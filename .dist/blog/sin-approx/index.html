<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="Bui Huy Giap's personal website and blog" name=description><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon><title>
      
        
          Approximating sin(2πx) | Zap's website
        
      
    </title><link href=/styles/fonts-critical.css rel=stylesheet><link href=/styles/fonts.css rel=stylesheet><style>h1{font-size:1.682rem}h2{font-size:1.414rem}h3{font-size:1.189rem}h4{font-size:1rem}h5{font-size:.841rem}h6{font-size:.707rem}</style><link href=/styles/base.css rel=stylesheet><link href=/styles/nav.css rel=stylesheet><link href=/styles/footer.css rel=stylesheet><link href=/rss.xml rel=alternate title=RSS type=application/rss+xml><link href=/styles/page.css rel=stylesheet><body><div class=nav__bg><div class=nav__container><nav class=nav><h3 class=nav__title><a class=nav__title__link href=/>Zap</a></h3><ul class=nav__links><li><a class=nav__link href=/works>Works</a><li><a class=nav__link href=/blog>Blog</a><li><a class=nav__link href=/about>About</a></ul></nav></div></div><div class=container><main class=content><header class=post-header><h1 class="post-header__title title">Approximating sin(2πx)</h1><div class=post-header__meta><div class=post-header__data><svg viewbox="0 0 448 512" height=1em xmlns=http://www.w3.org/2000/svg><path d="M128 0c17.7 0 32 14.3 32 32V64H288V32c0-17.7 14.3-32 32-32s32 14.3 32 32V64h48c26.5 0 48 21.5 48 48v48H0V112C0 85.5 21.5 64 48 64H96V32c0-17.7 14.3-32 32-32zM0 192H448V464c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V192zm64 80v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V272c0-8.8-7.2-16-16-16H80c-8.8 0-16 7.2-16 16zm128 0v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V272c0-8.8-7.2-16-16-16H208c-8.8 0-16 7.2-16 16zm144-16c-8.8 0-16 7.2-16 16v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V272c0-8.8-7.2-16-16-16H336zM64 400v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V400c0-8.8-7.2-16-16-16H80c-8.8 0-16 7.2-16 16zm144-16c-8.8 0-16 7.2-16 16v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V400c0-8.8-7.2-16-16-16H208zm112 16v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V400c0-8.8-7.2-16-16-16H336c-8.8 0-16 7.2-16 16z"/></svg> Tue, Nov 05 2024</div><div class=post-header__data><svg viewbox="0 0 512 512" height=1em xmlns=http://www.w3.org/2000/svg><path d="M256 0a256 256 0 1 1 0 512A256 256 0 1 1 256 0zM232 120V256c0 8 4 15.5 10.7 20l96 64c11 7.4 25.9 4.4 33.3-6.7s4.4-25.9-6.7-33.3L280 243.2V120c0-13.3-10.7-24-24-24s-24 10.7-24 24z"/></svg><abbr title="4434 words"> 18 minutes read </abbr></div></div></header><article class=content__body><p>I use trigonometric functions extremely often, ever since I started doing programming. They show up everywhere, especially in graphics, and while they’re not the most efficient operation, they are widely available in programming languages. This is not the case if you’re programming in embedded systems or <a href=/blog/wasm>WebAssembly</a>, so I tried to compute them myself.</p><span id=continue-reading></span><p>Trigonometric functions are actually quite similar in both their properties and their computation. So I’ll just write about how I computed sine, and the rest are similar. I actually prefer cosine, but I have chosen the sine function for this article because of the iconic sinusoidal wave visualization.<h1 id=why-sin-2px-and-not-sin-x>Why sin(2πx) and not sin(x)</h1><p>A big problem with general trigonometric functions is that they use radians as input (and as output for inverse trigonometric functions). Radians are exceptionally great for calculus but fall short when implemented on a computer. They make it so that common angles are expressed as multiples of pi, which don’t have perfectly accurate floating point representation. They also make computation much, much more complicated and less accurate.<p>So what is the alternative? Firstly, the concept of angle should be thought of as an abstract data type. The internal representation shouldn’t mean anything, and what gives them meaning are their operations. I can represent them with 32-bit unsigned integers where 0 is 0 degree and 2^32 (which is equal to 0) is 360 degree (which is also equal to 0 degree), and everything in between is interpolated. This method has a name, called BAM (Binary angular measurement), and was used in <a href=//doomwiki.org/wiki/Angle>Doom</a> back when floating-point arithmetic was too expensive.<p>While BAM is great, and I’m sure to investigate more into them, I’m not willing to implement some of the algorithm below in fixed-point or integer arithmetic. So a floating-point representation might be better. Degree is actually a pretty good contestant with its ability to represent many angles as integers, but it also has a problem similar to using radians when it comes to computing trigonometric functions.<h2 id=the-problem-with-degrees-and-radians>The problem with degrees and radians</h2><p>To understand the problem, let’s try to compute sine. Firstly, it’s very hard to design an approximation that works well in the entire range of input. What is commonly done is called argument reduction, where you exploit the properties of the function to transform the range of the input into something more manageable. For sine, it’s its periodicity, or, in more rigorous terms:<pre style=color:#dcdfe4;background-color:#282c34><code><span>sin(radians) = sin(radians ± 2π)
</span><span>sin(degrees) = sin(degrees ± 360)
</span></code></pre><p>This means that, for example, to compute sine in radians, we just need to compute sin in the range [0, 2π] and map the input to this range. This can be done by taking the remainder of x and 2π, which is <a href=//github.com/ziglang/zig/blob/master/lib/libc/musl/src/math/__rem_pio2.c#L49>extremely complicated</a> to perform efficiently and precisely. Recall that in radians, common angles are represented as multiples of pi, which means that to compute the sine of common angles, you have to first multiply by pi and then divide, wait for it, by pi. Although these two operations cancel each other, their errors still accumulate.<p>In the degree world, this problem is slightly mitigated as 360 is an integer, not a transcendental number, which helps a lot with precision and implementation complexity, but it’s not ideal. What if we have a representation that can perform argument reduction simply and effectively?<h2 id=enter-turn-the-measurement>Enter turn (the measurement)</h2><p>If I pick an unusual angle measurement, say 225 degrees, how do you make sense or quantify it? If you’re like me, you’d interpret it as being about 60% of a full angle. In CSS, a unit of angle measurement that uses multiples of a full angle is called a <a href=//developer.mozilla.org/en-US/docs/Web/CSS/angle#turn>turn</a>. For example, 720 degree is 2 turns and pi/4 is one eight of a turn. In my opinion, this thinking in terms of a full angle is more intuitive for both human and computer. 0.75 turns is more comprehensible than 270 degree, and to a certain extent, common angles can be represented exactly in binary floating-point. But what about argument reduction, here it is:<pre class=language-c data-lang=c style=color:#dcdfe4;background-color:#282c34><code class=language-c data-lang=c><span style=color:#c678dd>float</span><span> reduced </span><span style=color:#c678dd>=</span><span> x </span><span style=color:#c678dd>- </span><span>(</span><span style=color:#c678dd>int</span><span>)x </span><span style=color:#c678dd>+ </span><span>(x </span><span style=color:#c678dd>< </span><span style=color:#e5c07b>0</span><span>);
</span></code></pre><p>The <code>(int)x + (x < 0)</code> part can be simplified into a <code>floor(x)</code>, but casting the input to an integer will be handy in the future, and we don’t have to use <code>floor</code> from the math library. With degrees or radians, flooring does not suffice, and you have to use something like <code>fmod</code> or optimize it with complex bit manipulations like what most optimized implementations do. This is why I tried to compute <code>sin(2πx)</code>. <code>2πx</code> converts radians to turns, so <code>sin(2πx)</code> is simply <code>sin(x)</code>, but parameterized with turns instead of radians.<h1 id=different-ways-i-tried-to-compute-sine>Different ways I tried to compute sine</h1><p>With all the rationale behind turn out of the way, here’s my many attempts at computing sine. Unfortunately, this is where my expertise ended, and I was wandering in unknown territories. Just a disclaimer, I’m not formally educated in approximation and numerical algorithms, and I’m not the brightest at math, so the sections below will contain lots of hand-waving and little rigor.<h2 id=lookup-table-approach>Lookup table approach</h2><p>The easiest way to do something efficiently is to do it inefficiently once and store the results to look them up later. This is an approach I have used many times in the past and regularly reach for when solving a new problem.<figure><video autoplay loop muted><source src=lookup-vis.mp4 type=video/mp4></video><figcaption>An illustration of lookup table approximation with different values of sample points</figcaption></figure><p>To generate the lookup table (and for almost every other computation), I used the amazing <a href=//mpmath.org>mpmath library</a>.<pre class=language-python data-lang=python style=color:#dcdfe4;background-color:#282c34><code class=language-python data-lang=python><span style=color:#c678dd>import </span><span>mpmath
</span><span style=color:#c678dd>from </span><span>tqdm </span><span style=color:#c678dd>import </span><span>tqdm, trange
</span><span>
</span><span>mpmath.mp.prec </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>100
</span><span>
</span><span>total </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>1 </span><span style=color:#c678dd><< </span><span style=color:#e5c07b>24
</span><span>table </span><span style=color:#c678dd>= </span><span>[
</span><span>    mpmath.</span><span style=color:#e06c75>sin</span><span>(mpmath.pi </span><span style=color:#c678dd>* </span><span style=color:#e5c07b>2 </span><span style=color:#c678dd>* </span><span>x </span><span style=color:#c678dd>/ </span><span>total)
</span><span>    </span><span style=color:#c678dd>for </span><span>x </span><span style=color:#c678dd>in </span><span style=color:#e06c75>trange</span><span>(total </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>1</span><span>, </span><span style=color:#e06c75>desc</span><span style=color:#c678dd>=</span><span style=color:#98c379>"Generating lookup table"</span><span>)
</span><span>]
</span><span>
</span><span>filename </span><span style=color:#c678dd>= </span><span style=color:#98c379>"sin-lookup.c"
</span><span style=color:#c678dd>with </span><span style=color:#61afef>open</span><span>(filename, </span><span style=color:#98c379>"w"</span><span>) </span><span style=color:#c678dd>as </span><span>f:
</span><span>    f.</span><span style=color:#e06c75>write</span><span>(</span><span style=color:#98c379>"#include &LTstddef.h></span><span style=color:#56b6c2>\n</span><span style=color:#98c379>"</span><span>)
</span><span>    f.</span><span style=color:#e06c75>write</span><span>(</span><span style=color:#c678dd>f</span><span style=color:#98c379>"const size_t SIN_TABLE_RANGE = </span><span>{total}</span><span style=color:#98c379>;</span><span style=color:#56b6c2>\n</span><span style=color:#98c379>"</span><span>)
</span><span>    f.</span><span style=color:#e06c75>write</span><span>(</span><span style=color:#c678dd>f</span><span style=color:#98c379>"const float SIN_TABLE[</span><span>{total </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>1</span><span>}</span><span style=color:#98c379>] = </span><span style=color:#56b6c2>{{\n</span><span style=color:#98c379>"</span><span>)
</span><span>    </span><span style=color:#c678dd>for </span><span>entry </span><span style=color:#c678dd>in </span><span style=color:#e06c75>tqdm</span><span>(table, </span><span style=color:#e06c75>desc</span><span style=color:#c678dd>=f</span><span style=color:#98c379>"Writing lookup table to `</span><span>{filename}</span><span style=color:#98c379>`"</span><span>):
</span><span>        f.</span><span style=color:#e06c75>write</span><span>(</span><span style=color:#c678dd>f</span><span style=color:#98c379>"  </span><span>{entry}</span><span style=color:#98c379>,</span><span style=color:#56b6c2>\n</span><span style=color:#98c379>"</span><span>)
</span><span>    f.</span><span style=color:#e06c75>write</span><span>(</span><span style=color:#98c379>"};</span><span style=color:#56b6c2>\n</span><span style=color:#98c379>"</span><span>)
</span></code></pre><p>On my machine, generating the lookup table took 3 minutes, writing <code>sin-lookup.c</code> took 50 seconds and compiling with <code>clang -c -o sin-lookup.o sin-lookup.c</code> took 40 seconds. Which clocks in at a total of 4 minutes and 30 seconds to compute.<p>Notice that I added a value to the end because argument reduction converts positive values to [0, 1) and negative values to (0, 1], so the entire range of input is converted to [0, 1]. Multiplied with the range, the input will become [0, range] so we need a lookup table of size range + 1.<p>Computing sine with the generated lookup table is easy, just multiply the reduced input with the range and index the rounded result into the lookup table. I also added a reduced range variant to emulate using smaller lookup tables.<pre class=language-c data-lang=c style=color:#dcdfe4;background-color:#282c34><code class=language-c data-lang=c><span style=color:#c678dd>#include </span><span style=color:#98c379>&LTstdint.h>
</span><span style=color:#c678dd>#include </span><span style=color:#98c379>&LTstddef.h>
</span><span>
</span><span style=color:#c678dd>extern const int</span><span> SIN_TABLE_RANGE;
</span><span style=color:#c678dd>extern const float</span><span> SIN_TABLE[];
</span><span>
</span><span style=color:#c678dd>float </span><span style=color:#61afef>sin_lookup</span><span>(</span><span style=color:#c678dd>float </span><span style=color:#e06c75>x</span><span>) {
</span><span>    x </span><span style=color:#c678dd>-= </span><span>(int32_t)x </span><span style=color:#c678dd>- </span><span>(x </span><span style=color:#c678dd>< </span><span style=color:#e5c07b>0</span><span>);
</span><span>    size_t entry </span><span style=color:#c678dd>=</span><span> x </span><span style=color:#c678dd>*</span><span> SIN_TABLE_RANGE </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.5</span><span style=color:#c678dd>f</span><span>;
</span><span>    </span><span style=color:#c678dd>return</span><span> SIN_TABLE[entry];
</span><span>}
</span><span>
</span><span style=color:#c678dd>float </span><span style=color:#61afef>sin_lookup_reduced</span><span>(</span><span style=color:#c678dd>float </span><span style=color:#e06c75>x</span><span>, size_t </span><span style=color:#e06c75>reduced_range</span><span>) {
</span><span>    x </span><span style=color:#c678dd>-= </span><span>(int32_t)x </span><span style=color:#c678dd>- </span><span>(x </span><span style=color:#c678dd>< </span><span style=color:#e5c07b>0</span><span>);
</span><span>    size_t entry </span><span style=color:#c678dd>= </span><span>(size_t)(x </span><span style=color:#c678dd>*</span><span> reduced_range </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.5</span><span style=color:#c678dd>f</span><span>) </span><span style=color:#c678dd>*</span><span> SIN_TABLE_RANGE </span><span style=color:#c678dd>/</span><span> reduced_range;
</span><span>    </span><span style=color:#c678dd>return</span><span> SIN_TABLE[entry];
</span><span>}
</span></code></pre><p>Using lookup tables is very accurate, but other than being horrendously slow to compute, it also generates a <code>64mb</code> binary. And while looking up the table is cheap, the large table size can cause cache misses, which is bad for performance when used in a computational pipeline. However, the lookup table gives me a good reference implementation for me to test other algorithms. Before that, I tested the correlation between table size and precision for this method.<h2 id=testing-framework>Testing framework</h2><p>The correct way to test a numerical method is to test it on all floating-point representation of the input range. Because we reduced the input to [0, 1], we need to test all floating-point representation of numbers between 0 and 1, which has <code>2^30</code> unique values. This is too big for me, so I’ll just use the <code>2^23</code> values in the [1, 2] range, which is uniformly distributed and matches the range of the lookup table. For completeness, I’ll also subtract the range by 1 to get a subset of the [0, 1] range and test those too.<pre class=language-c data-lang=c style=color:#dcdfe4;background-color:#282c34><code class=language-c data-lang=c><span style=color:#c678dd>float </span><span style=color:#61afef>compute_error</span><span>(</span><span style=color:#c678dd>float </span><span style=color:#e06c75>sin_impl</span><span>(</span><span style=color:#c678dd>float</span><span>)) {
</span><span>    </span><span style=color:#c678dd>float</span><span> max_error </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>0</span><span>;
</span><span>    </span><span style=color:#c678dd>for </span><span>(uint32_t mantissa </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>0</span><span>; mantissa </span><span style=color:#c678dd>< </span><span style=color:#e5c07b>1</span><span style=color:#c678dd>u << </span><span style=color:#e5c07b>23</span><span>; </span><span style=color:#c678dd>++</span><span>mantissa) {
</span><span>        </span><span style=color:#c678dd>union </span><span>{
</span><span>            uint32_t u;
</span><span>            </span><span style=color:#c678dd>float</span><span> f;
</span><span>        } buf </span><span style=color:#c678dd>= </span><span>{ .</span><span style=color:#e06c75>f </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>1 </span><span>};
</span><span>
</span><span>        buf.</span><span style=color:#e06c75>u </span><span style=color:#c678dd>|=</span><span> mantissa;
</span><span>
</span><span>        </span><span style=color:#5c6370>// x of range [1, 2]
</span><span>        </span><span style=color:#c678dd>float</span><span> x1 </span><span style=color:#c678dd>=</span><span> buf.</span><span style=color:#e06c75>f</span><span>;
</span><span>
</span><span>        </span><span style=color:#5c6370>// x of range [0, 1]
</span><span>        </span><span style=color:#c678dd>float</span><span> x2 </span><span style=color:#c678dd>=</span><span> buf.</span><span style=color:#e06c75>f </span><span style=color:#c678dd>- </span><span style=color:#e5c07b>1</span><span>;
</span><span>
</span><span>        </span><span style=color:#c678dd>float</span><span> target1 </span><span style=color:#c678dd>= </span><span style=color:#e06c75>sin_lookup</span><span>(x1);
</span><span>        </span><span style=color:#c678dd>float</span><span> target2 </span><span style=color:#c678dd>= </span><span style=color:#e06c75>sin_lookup</span><span>(x2);
</span><span>
</span><span>        </span><span style=color:#c678dd>float</span><span> result1 </span><span style=color:#c678dd>= </span><span style=color:#e06c75>sin_impl</span><span>(x1);
</span><span>        </span><span style=color:#c678dd>float</span><span> result2 </span><span style=color:#c678dd>= </span><span style=color:#e06c75>sin_impl</span><span>(x2);
</span><span>
</span><span>        </span><span style=color:#5c6370>// Compute the absolute error
</span><span>        </span><span style=color:#c678dd>float</span><span> error1 </span><span style=color:#c678dd>=</span><span> result1 </span><span style=color:#c678dd>></span><span> target1 </span><span style=color:#c678dd>?</span><span> result1 </span><span style=color:#c678dd>-</span><span> target1 </span><span style=color:#c678dd>:</span><span> target1 </span><span style=color:#c678dd>-</span><span> result1;
</span><span>        </span><span style=color:#c678dd>float</span><span> error2 </span><span style=color:#c678dd>=</span><span> result2 </span><span style=color:#c678dd>></span><span> target2 </span><span style=color:#c678dd>?</span><span> result2 </span><span style=color:#c678dd>-</span><span> target2 </span><span style=color:#c678dd>:</span><span> target2 </span><span style=color:#c678dd>-</span><span> result2;
</span><span>
</span><span>        </span><span style=color:#5c6370>// Update the max error
</span><span>        </span><span style=color:#c678dd>if </span><span>(error1 </span><span style=color:#c678dd>></span><span> max_error) max_error </span><span style=color:#c678dd>=</span><span> error1;
</span><span>        </span><span style=color:#c678dd>if </span><span>(error2 </span><span style=color:#c678dd>></span><span> max_error) max_error </span><span style=color:#c678dd>=</span><span> error2;
</span><span>    }
</span><span>
</span><span>    </span><span style=color:#c678dd>return</span><span> max_error </span><span style=color:#c678dd>/</span><span> FLT_EPSILON;
</span><span>}
</span></code></pre><p>The final line <code>return max_error / FLT_EPSILON</code> converts the maximum error into a unit called <a href=//en.wikipedia.org/wiki/Unit_in_the_last_place>ULP</a>. The reason is that 5 ULP and 10 ULP are easier to display than 5.960464477539062e-07 and 1.1920928955078125e-06. Also, ULP takes into account of the data type, so while 5.960464477539062e-07 is 5 ULP in 32-bit float which is great, but it is a whopping 2684354560 ULP in 64-bit float which is absolutely horrible.<h3 id=testing-smaller-lookup-tables>Testing smaller lookup tables</h3><figure><img alt="Lookup tables result" src=lookup-result.svg><figcaption>Results of smaller lookup tables</figcaption></figure><p>From the figure above, we can see that the error is inversely proportional to the table size, which is quite predictable. The noise in the graph is due to the error when interpolating the smaller table when the size is not a power of two. Although the method converges quite quickly, to reach high enough accuracy, you need very large lookup tables. For example, to reach sub-20 ULP, the lookup table needs to have the size of 2^21 or about 2 millions elements. And the method definitely won’t scale well when applied to double-precision floats.<h3 id=testing-standard-library-sine>Testing standard library sine</h3><p>To verify my claim about the accumulating error when computing sine, let’s test the standard library implementation of sine on multiple ranges of input, using the lookup table as our target output.<figure><img alt="STD implementation result" src=std-result.svg><figcaption>Results of standard library implementation of sin(2πx)</figcaption></figure><p>As you can see, the error of the standard implementation increases as we go further away from 0. This confirms the accumulating error of multiplying by 2πx and then taking the remainder. You might be skeptical as I’m using the lookup table as a reference, so here’s the result of <code>sin(2πx)</code> in ULP for x in [0, 1000]. Of course, the correct results are zeros, but let’s see what the standard library gives.<figure><img alt="STD implementation error" src=std-error.svg><figcaption>sin(2πx) according to the glibc standard library</figcaption></figure><p>Okay, that’s quite insane, the error at 1000 is 2010 ULP in 32-bit float which is 2.4e-3, a number that is definitely not insignificant. This is why we shouldn’t use radians for numerically computing trigonometric functions.<h2 id=improving-the-lookup-table-approach>Improving the lookup table approach</h2><p>Before trying out other forms of computing sine, I tried to exploit the symmetry of sine to improve the lookup table method. Recall that:<pre style=color:#dcdfe4;background-color:#282c34><code><span>sin(radians) = -sin(radians - π)
</span><span>sin(turns) = -sin(turns - 0.5)
</span></code></pre><p>So, instead of computing sin in the range [0, 1], we can just compute sin in the range [0, 0.5] and negate the result if x is in the range [0.5, 1]. It’s more efficient to multiply x by 2 before reduce the argument, compute <code>sin(πx)</code> on the reduced argument and check if the original is divisible by 2.<pre class=language-c data-lang=c style=color:#dcdfe4;background-color:#282c34><code class=language-c data-lang=c><span style=color:#c678dd>extern const int</span><span> SIN_TABLE_RANGE;
</span><span style=color:#c678dd>extern const float</span><span> SIN_TABLE[];
</span><span>
</span><span style=color:#c678dd>float </span><span style=color:#61afef>sin_lookup</span><span>(</span><span style=color:#c678dd>float </span><span style=color:#e06c75>x</span><span>) {
</span><span>    x </span><span style=color:#c678dd>*= </span><span style=color:#e5c07b>2</span><span>;
</span><span>    int32_t ix </span><span style=color:#c678dd>= </span><span>(int32_t)x </span><span style=color:#c678dd>- </span><span>(x </span><span style=color:#c678dd>< </span><span style=color:#e5c07b>0</span><span>);
</span><span>    x </span><span style=color:#c678dd>-=</span><span> ix;
</span><span>    size_t entry </span><span style=color:#c678dd>=</span><span> x </span><span style=color:#c678dd>*</span><span> SIN_TABLE_RANGE </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.5</span><span style=color:#c678dd>f</span><span>;
</span><span>    </span><span style=color:#c678dd>float</span><span> y </span><span style=color:#c678dd>=</span><span> SIN_TABLE[entry];
</span><span>    </span><span style=color:#c678dd>if </span><span>(ix </span><span style=color:#c678dd>& </span><span style=color:#e5c07b>1</span><span>) y </span><span style=color:#c678dd>= -</span><span>y;
</span><span>    </span><span style=color:#c678dd>return</span><span> y;
</span><span>}
</span></code></pre><p>If you use half-turn instead of turn, you have effectively reduced an instruction from the algorithm, but multiplying by two is essentially adding with itself, and I feel like the inputs are more likely to be in the range in [0, 1) so you have to multiply by 2 anyway. This effectively reduced the memory usage by half for the same precision, as shown in the figure below.<figure><img alt="Lookup tables optimized result" src=lookup-result1.svg><figcaption>Optimized lookup table compared to the original</figcaption></figure><p>Or you can look at it in another way: By exploiting the symmetry of sine, we doubled the precision of the same algorithm, for free. There are ways to exploit even more symmetry, but we’ll look at other methods of implementing sine first.<h2 id=taylor-polynomials>Taylor polynomials</h2><p>Computers don’t understand sine and cosine, but they understand polynomials, so we can use polynomials to approximate sine. The most basic way of doing this —which you might have learned from your college calculus course—is to use a truncated Taylor series. At this point, there will be a lot of math, so I’ll switch to exclusively using radians to explain what I did.<figure><video autoplay loop muted><source src=taylor-vis.mp4 type=video/mp4></video><figcaption>An illustration of Taylor series approximation with different polynomial degrees</figcaption></figure><p>Since we’re computing sin(πx) in the range [0, 1], it’s a good idea to compute the Taylor series around 0.5. A benefit of using Taylor series is that the highest error typically resides at the boundary, i.e., 0 and 1, so we can compute the error at those points and compare them to the error observed in the benchmark. Mpmath already has support for Taylor series, but the function is simple enough for me to calculate myself.<figure><img alt="taylor series" src=taylor.svg><figcaption>Taylor series of sin(πx) around x = 0.5</figcaption></figure><p>If the origin is at x = 0.5, sin(πx) becomes an even function, so its power series consists only of even-degree monomials. Translating this to code is very straightforward:<pre class=language-python data-lang=python style=color:#dcdfe4;background-color:#282c34><code class=language-python data-lang=python><span style=color:#c678dd>import </span><span>mpmath
</span><span>mpmath.mp.prec </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>100
</span><span>
</span><span>total </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>10
</span><span>sign </span><span style=color:#c678dd>= </span><span>[</span><span style=color:#e5c07b>1</span><span>, </span><span style=color:#e5c07b>0</span><span>, </span><span style=color:#c678dd>-</span><span style=color:#e5c07b>1</span><span>, </span><span style=color:#e5c07b>0</span><span>] </span><span style=color:#5c6370># cos(pi * n / 2)
</span><span>
</span><span>n </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>1
</span><span>d </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>1
</span><span>
</span><span>coeffs </span><span style=color:#c678dd>= </span><span>[mpmath.</span><span style=color:#e06c75>mpmathify</span><span>(</span><span style=color:#e5c07b>1</span><span>)]
</span><span>
</span><span>idx </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>1
</span><span style=color:#c678dd>while </span><span style=color:#61afef>len</span><span>(coeffs) </span><span style=color:#c678dd>< </span><span>total:
</span><span>    n </span><span style=color:#c678dd>*= </span><span>mpmath.pi
</span><span>    d </span><span style=color:#c678dd>*= </span><span>mpmath.</span><span style=color:#e06c75>mpmathify</span><span>(idx)
</span><span>    </span><span style=color:#c678dd>if </span><span>sign[idx </span><span style=color:#c678dd>% </span><span style=color:#e5c07b>4</span><span>] </span><span style=color:#c678dd>!= </span><span style=color:#e5c07b>0</span><span>:
</span><span>        coeffs.</span><span style=color:#e06c75>append</span><span>(n </span><span style=color:#c678dd>* </span><span>sign[idx </span><span style=color:#c678dd>% </span><span style=color:#e5c07b>4</span><span>] </span><span style=color:#c678dd>/ </span><span>d)
</span><span>    idx </span><span style=color:#c678dd>+= </span><span style=color:#e5c07b>1
</span><span>
</span><span style=color:#c678dd>for </span><span>coeff </span><span style=color:#c678dd>in </span><span>coeffs:
</span><span>    </span><span style=color:#61afef>print</span><span>(</span><span style=color:#c678dd>f</span><span style=color:#98c379>"</span><span>{coeff}</span><span style=color:#98c379>,"</span><span>)
</span></code></pre><p>That <code>,</code> at the end is for easier copying into C code. Unlike the lookup table approach, this runs instantly, and produces a very small set of polynomial coefficients. In the C code, we can evaluate the polynomial using Horner’s method:<pre class=language-c data-lang=c style=color:#dcdfe4;background-color:#282c34><code class=language-c data-lang=c><span style=color:#c678dd>float </span><span style=color:#61afef>sin_taylor</span><span>(</span><span style=color:#c678dd>float </span><span style=color:#e06c75>x</span><span>, </span><span style=color:#c678dd>int </span><span style=color:#e06c75>n</span><span>) {
</span><span>    </span><span style=color:#c678dd>static const float</span><span> coeffs[] </span><span style=color:#c678dd>= </span><span>{
</span><span>        </span><span style=color:#5c6370>/* ... */
</span><span>    };
</span><span>
</span><span>    x </span><span style=color:#c678dd>*= </span><span style=color:#e5c07b>2</span><span>;
</span><span>    int32_t ix </span><span style=color:#c678dd>= </span><span>(int32_t)x </span><span style=color:#c678dd>- </span><span>(x </span><span style=color:#c678dd>< </span><span style=color:#e5c07b>0</span><span>);
</span><span>    x </span><span style=color:#c678dd>-=</span><span> ix </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.5</span><span style=color:#c678dd>f</span><span>; </span><span style=color:#5c6370>// also subtracts x by 0.5 
</span><span>    </span><span style=color:#c678dd>float</span><span> xx </span><span style=color:#c678dd>=</span><span> x </span><span style=color:#c678dd>*</span><span> x;
</span><span>    </span><span style=color:#c678dd>float</span><span> y </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>0</span><span>;
</span><span>    </span><span style=color:#c678dd>for </span><span>(</span><span style=color:#c678dd>int</span><span> i </span><span style=color:#c678dd>=</span><span> n </span><span style=color:#c678dd>- </span><span style=color:#e5c07b>1</span><span>; i </span><span style=color:#c678dd>>= </span><span style=color:#e5c07b>0</span><span>; </span><span style=color:#c678dd>--</span><span>i) {
</span><span>        y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+</span><span> coeffs[i];
</span><span>    }
</span><span>    </span><span style=color:#c678dd>if </span><span>(ix </span><span style=color:#c678dd>& </span><span style=color:#e5c07b>1</span><span>) y </span><span style=color:#c678dd>= -</span><span>y;
</span><span>    </span><span style=color:#c678dd>return</span><span> y;
</span><span>}
</span></code></pre><p>In production code, it’s better to inline the polynomial evaluation instead of using for loop. But with this code, I could test the error of each truncation point for the series. Because the power series consists of only even-degree monomials, every step of Horner’s method the result is multiplied by x^2 instead of x. This means that I essentially got double the polynomial degree for free, which is another aspect of exploiting the symmetry of the sine function.<figure><img alt="Taylor series result" src=taylor-result.svg><figcaption>Results of Taylor series approximation with different truncation points</figcaption></figure><p>The figure above shows the error of Taylor series approximation, both from running the benchmark at 32-bit floating-point resolution and from computing <code>sin(π)</code> and <code>sin(0)</code> with the polynomial in mpmath. The result confirmed my hypothesis that the maximum error of the approximation is at the boundary, at least until the precision ran out.<p>After degree 14, while the theoretical error keeps decreasing, the actual error from the benchmark stuck at 0.974121. This is possibly due to the multiplication and addition error accumulated during Horner’s method iteration. This can be counteracted by computing the polynomial in higher precision before rounding back to 32-bit. Doing this actually allowed me to achieve perfect accuracy (0 ULP) for the degree 20 polynomial.<p>But precision is not everything, and I also cared about optimizing the number of operations while still retaining high precision. The degree 10 polynomial, which has only 6 coefficients, has the error of 4.36 ULP, which is better than the optimized lookup table approach of size 2^21 (with the error of 6.31 ULP). This is good, but I wanted to reach the saturation point faster.<h2 id=pade-approximation>Padé approximation</h2><p>A simple way to improve the Taylor approximation is to convert the truncated power series into a rational polynomial. This is known as the Padé approximant. This method simply solves a system of equations to obtain a rational polynomial that agrees with the original function. It feels magical and cheating to get a better approximation with the same function by transforming it, but a rational polynomial can better model the asymptotic behaviour of some functions.<p>Like for the taylor series, mpmath has support for generating Padé approximations, and I didn’t do it myself this time, unlike the Taylor approximation, because the procedure is different for each order of the polynomial. After getting the coefficients, I used some metaprogramming to generate inlined polynomial evaluations. Since the original series has only even-degree monomials, its rational approximations also have even-degree numerators and denominators. So I just skipped every other coefficient for both and performed Horner’s method on strings to generate the polynomial.<pre class=language-python data-lang=python style=color:#dcdfe4;background-color:#282c34><code class=language-python data-lang=python><span style=color:#c678dd>import </span><span>mpmath
</span><span>mpmath.mp.prec </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>100
</span><span>
</span><span>total </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>21
</span><span>sign </span><span style=color:#c678dd>= </span><span>[</span><span style=color:#e5c07b>1</span><span>, </span><span style=color:#e5c07b>0</span><span>, </span><span style=color:#c678dd>-</span><span style=color:#e5c07b>1</span><span>, </span><span style=color:#e5c07b>0</span><span>] </span><span style=color:#5c6370># cos(pi n)
</span><span>
</span><span>n </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>1
</span><span>d </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>1
</span><span>
</span><span>coeffs </span><span style=color:#c678dd>= </span><span>[mpmath.</span><span style=color:#e06c75>mpmathify</span><span>(</span><span style=color:#e5c07b>1</span><span>)]
</span><span>
</span><span style=color:#c678dd>for </span><span>i </span><span style=color:#c678dd>in </span><span style=color:#61afef>range</span><span>(</span><span style=color:#e5c07b>1</span><span>, total):
</span><span>    n </span><span style=color:#c678dd>*= </span><span>mpmath.pi
</span><span>    d </span><span style=color:#c678dd>*= </span><span>mpmath.</span><span style=color:#e06c75>mpmathify</span><span>(i)
</span><span>    coeffs.</span><span style=color:#e06c75>append</span><span>(n </span><span style=color:#c678dd>* </span><span>sign[i </span><span style=color:#c678dd>% </span><span style=color:#e5c07b>4</span><span>] </span><span style=color:#c678dd>/ </span><span>d)
</span><span>
</span><span style=color:#c678dd>for </span><span>i </span><span style=color:#c678dd>in </span><span style=color:#61afef>range</span><span>(</span><span style=color:#e5c07b>1</span><span>, </span><span style=color:#e5c07b>11</span><span>):
</span><span>    p_coeffs, q_coeffs </span><span style=color:#c678dd>= </span><span>mpmath.</span><span style=color:#e06c75>pade</span><span>(coeffs[:i </span><span style=color:#c678dd>+ </span><span>i], i, i </span><span style=color:#c678dd>- </span><span style=color:#e5c07b>1</span><span>)
</span><span>    
</span><span>    p_coeffs </span><span style=color:#c678dd>= </span><span>p_coeffs[::</span><span style=color:#e5c07b>2</span><span>][::</span><span style=color:#c678dd>-</span><span style=color:#e5c07b>1</span><span>]
</span><span>    q_coeffs </span><span style=color:#c678dd>= </span><span>q_coeffs[::</span><span style=color:#e5c07b>2</span><span>][::</span><span style=color:#c678dd>-</span><span style=color:#e5c07b>1</span><span>]
</span><span>
</span><span>    p </span><span style=color:#c678dd>= f</span><span style=color:#98c379>"</span><span>{p_coeffs[</span><span style=color:#e5c07b>0</span><span>]}</span><span style=color:#98c379>"
</span><span>    </span><span style=color:#c678dd>for </span><span>p_coeff </span><span style=color:#c678dd>in </span><span>p_coeffs[</span><span style=color:#e5c07b>1</span><span>:]:
</span><span>        p </span><span style=color:#c678dd>= f</span><span style=color:#98c379>"(</span><span>{p}</span><span style=color:#98c379> * xx + </span><span>{p_coeff}</span><span style=color:#98c379>)"
</span><span>
</span><span>    q </span><span style=color:#c678dd>= f</span><span style=color:#98c379>"</span><span>{q_coeffs[</span><span style=color:#e5c07b>0</span><span>]}</span><span style=color:#98c379>"
</span><span>    </span><span style=color:#c678dd>for </span><span>q_coeff </span><span style=color:#c678dd>in </span><span>q_coeffs[</span><span style=color:#e5c07b>1</span><span>:]:
</span><span>        q </span><span style=color:#c678dd>= f</span><span style=color:#98c379>"(</span><span>{q}</span><span style=color:#98c379> * xx + </span><span>{q_coeff}</span><span style=color:#98c379>)"
</span><span>
</span><span>    </span><span style=color:#61afef>print</span><span>(</span><span style=color:#c678dd>f</span><span style=color:#98c379>"p = </span><span>{p}</span><span style=color:#98c379>"</span><span>)
</span><span>    </span><span style=color:#61afef>print</span><span>(</span><span style=color:#c678dd>f</span><span style=color:#98c379>"q = </span><span>{q}</span><span style=color:#98c379>"</span><span>)
</span></code></pre><p>Here’s an example of the order [6/6] Padé approximation in C. It agrees with the Taylor series of order 12, but in terms of computation, it’s more expensive and less precise because of the extra division.<pre class=language-c data-lang=c style=color:#dcdfe4;background-color:#282c34><code class=language-c data-lang=c><span style=color:#c678dd>float </span><span style=color:#61afef>sin_pade_6_6</span><span>(</span><span style=color:#c678dd>float </span><span style=color:#e06c75>x</span><span>) {
</span><span>    x </span><span style=color:#c678dd>*= </span><span style=color:#e5c07b>2</span><span>;
</span><span>    int32_t ix </span><span style=color:#c678dd>= </span><span>(int32_t)x </span><span style=color:#c678dd>- </span><span>(x </span><span style=color:#c678dd>< </span><span style=color:#e5c07b>0</span><span>);
</span><span>    x </span><span style=color:#c678dd>-=</span><span> ix </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.5</span><span style=color:#c678dd>f</span><span>;
</span><span>    </span><span style=color:#c678dd>float</span><span> xx </span><span style=color:#c678dd>=</span><span> x </span><span style=color:#c678dd>*</span><span> x;
</span><span>
</span><span>    </span><span style=color:#c678dd>float</span><span> p </span><span style=color:#c678dd>= -</span><span style=color:#e5c07b>0.35796583327481520071570877176</span><span style=color:#c678dd>f *</span><span> xx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>2.6678684023565382463447255987</span><span style=color:#c678dd>f</span><span>;
</span><span>    p </span><span style=color:#c678dd>=</span><span> p </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ -</span><span style=color:#e5c07b>4.6445942642517333510565497579</span><span style=color:#c678dd>f</span><span>;
</span><span>    p </span><span style=color:#c678dd>=</span><span> p </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>1.0</span><span style=color:#c678dd>f</span><span>;
</span><span>
</span><span>    </span><span style=color:#c678dd>float</span><span> q </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>0.003110616546418168353807390627</span><span style=color:#c678dd>f *</span><span> xx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.041275038573729846286627259614</span><span style=color:#c678dd>f</span><span>;
</span><span>    q </span><span style=color:#c678dd>=</span><span> q </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.29020793629294595836069574203</span><span style=color:#c678dd>f</span><span>;
</span><span>    q </span><span style=color:#c678dd>=</span><span> q </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>1.0</span><span style=color:#c678dd>f</span><span>;
</span><span>
</span><span>    </span><span style=color:#c678dd>float</span><span> y </span><span style=color:#c678dd>=</span><span> p </span><span style=color:#c678dd>/</span><span> q;
</span><span>    </span><span style=color:#c678dd>if </span><span>(ix </span><span style=color:#c678dd>& </span><span style=color:#e5c07b>1</span><span>) y </span><span style=color:#c678dd>= -</span><span>y;
</span><span>    </span><span style=color:#c678dd>return</span><span> y;
</span><span>}
</span></code></pre><p>A low-level advantage of rational polynomial approximation over polynomial approximation is that the numerators and denominators can be computed independently, which allows for better vectorization and pipelining, even for scalar value. Compiling with <code>clang -O3 -march=native</code> on my AMD Ryzen 7 5800U, the following assembly code is generated:<pre class=language-asm data-lang=asm style=color:#dcdfe4;background-color:#282c34><code class=language-asm data-lang=asm><span style=color:#61afef>sin_pade_6_6:
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vaddss          </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm0
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vxorps          </span><span style=color:#e06c75>xmm1</span><span>, </span><span style=color:#e06c75>xmm1</span><span>, </span><span style=color:#e06c75>xmm1
</span><span style=color:#61afef>    </span><span style=color:#c678dd>xor             </span><span style=color:#e06c75>ecx</span><span>, </span><span style=color:#e06c75>ecx
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vbroadcastss    </span><span style=color:#e06c75>xmm2</span><span>, </span><span style=color:#61afef>dword ptr </span><span>[</span><span style=color:#e06c75>rip </span><span>+ </span><span style=color:#61afef>.LCPI0_4</span><span>]
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vucomiss        </span><span style=color:#e06c75>xmm1</span><span>, </span><span style=color:#e06c75>xmm0
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vcvttss2si      </span><span style=color:#e06c75>eax</span><span>, </span><span style=color:#e06c75>xmm0
</span><span style=color:#61afef>    </span><span style=color:#c678dd>seta            </span><span style=color:#e06c75>cl
</span><span style=color:#61afef>    </span><span style=color:#c678dd>sub             </span><span style=color:#e06c75>eax</span><span>, </span><span style=color:#e06c75>ecx
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vcvtsi2ss       </span><span style=color:#e06c75>xmm1</span><span>, </span><span style=color:#e06c75>xmm3</span><span>, </span><span style=color:#e06c75>eax
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vaddss          </span><span style=color:#e06c75>xmm1</span><span>, </span><span style=color:#e06c75>xmm1</span><span>, </span><span style=color:#61afef>dword ptr </span><span>[</span><span style=color:#e06c75>rip </span><span>+ </span><span style=color:#61afef>.LCPI0_0</span><span>]
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vsubss          </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm1
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vmovsd          </span><span style=color:#e06c75>xmm1</span><span>, </span><span style=color:#61afef>qword ptr </span><span>[</span><span style=color:#e06c75>rip </span><span>+ </span><span style=color:#61afef>.LCPI0_6</span><span>]
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vmulss          </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm0
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vbroadcastss    </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm0
</span><span style=color:#61afef>    vfmadd213ps     </span><span style=color:#e06c75>xmm1</span><span>, </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#61afef>xmmword ptr </span><span>[</span><span style=color:#e06c75>rip </span><span>+ </span><span style=color:#61afef>.LCPI0_2</span><span>]
</span><span style=color:#61afef>    vfmadd213ps     </span><span style=color:#e06c75>xmm1</span><span>, </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#61afef>xmmword ptr </span><span>[</span><span style=color:#e06c75>rip </span><span>+ </span><span style=color:#61afef>.LCPI0_3</span><span>]
</span><span style=color:#61afef>    vfmadd231ps     </span><span style=color:#e06c75>xmm2</span><span>, </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm1
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vmovshdup       </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm2
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vdivss          </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm2</span><span>, </span><span style=color:#e06c75>xmm0
</span><span style=color:#61afef>    </span><span style=color:#c678dd>test            </span><span style=color:#e06c75>al</span><span>, </span><span style=color:#e5c07b>1
</span><span style=color:#61afef>    </span><span style=color:#c678dd>je              </span><span style=color:#61afef>.LBB0_2
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vbroadcastss    </span><span style=color:#e06c75>xmm1</span><span>, </span><span style=color:#61afef>dword ptr </span><span>[</span><span style=color:#e06c75>rip </span><span>+ </span><span style=color:#61afef>.LCPI0_5</span><span>]
</span><span style=color:#61afef>    </span><span style=color:#c678dd>vxorps          </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm0</span><span>, </span><span style=color:#e06c75>xmm1
</span><span style=color:#61afef>.LBB0_2:
</span><span style=color:#61afef>    </span><span style=color:#c678dd>ret
</span></code></pre><p>As you can see, the generated routine compute two polynomial with just 3 SIMD fused multiply-add operations, each on a vector of 2 floats. This efficiency somewhat made up for the extra division, but let’s see how well this does in terms of precision.<figure><img alt="Pade approximation result" src=pade-result.svg><figcaption>Comparison of Taylor and Padé approximation</figcaption></figure><p>Although Padé approximation is slightly better when not limited by floating-point precision, it’s worse in the benchmark because of the division error. This shows that although Padé approximation is considered better than truncated Taylor series, it’s actually a worse numerical approximation method in limited precision settings. Despite this, the independent computations of numerators and denominators are great and might help with performance.<h2 id=chebyshev-polynomials>Chebyshev polynomials</h2><p>The problem with truncated Taylor series is also what allowed me to compute the maximum error easily: the further away from the center of the approximation, the bigger the error. This means that the error does not distribute uniformly across the input range. So Taylor polynomials are good at approximating a function around a point, not across a range.<figure><video autoplay loop muted><source src=cheby-vis.mp4 type=video/mp4></video><figcaption>An illustration of Chebyshev approximation with different polynomial degrees</figcaption></figure><p>One method of finding approximating polynomials across a range is called Chebyshev approximation. I couldn’t wrap my head around the theory behind it, so I just let mpmath do the heavy lifting. Unlike Taylor series, where you can just truncate the polynomial to get a lower degree approximation, with Chebyshev polynomials, you have to make one for every degree.<pre class=language-python data-lang=python style=color:#dcdfe4;background-color:#282c34><code class=language-python data-lang=python><span style=color:#c678dd>import </span><span>mpmath
</span><span style=color:#c678dd>import </span><span>numpy </span><span style=color:#c678dd>as </span><span>np
</span><span>
</span><span>mpmath.mp.prec </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>100
</span><span>
</span><span>eps </span><span style=color:#c678dd>= </span><span>np.</span><span style=color:#e06c75>finfo</span><span>(np.float32).eps
</span><span>
</span><span style=color:#c678dd>def </span><span style=color:#61afef>fn</span><span>(</span><span style=color:#e06c75>x</span><span>):
</span><span>    </span><span style=color:#c678dd>return </span><span>mpmath.</span><span style=color:#e06c75>sin</span><span>(mpmath.pi </span><span style=color:#c678dd>* </span><span>x)
</span><span>
</span><span style=color:#c678dd>for </span><span>degree </span><span style=color:#c678dd>in </span><span style=color:#61afef>range</span><span>(</span><span style=color:#e5c07b>1</span><span>, </span><span style=color:#e5c07b>20</span><span>):
</span><span>    poly, err </span><span style=color:#c678dd>= </span><span>mpmath.</span><span style=color:#e06c75>chebyfit</span><span>(fn, [</span><span style=color:#e5c07b>0</span><span>, </span><span style=color:#e5c07b>1</span><span>], degree, </span><span style=color:#e06c75>error</span><span style=color:#c678dd>=</span><span style=color:#e5c07b>True</span><span>)
</span><span>    poly </span><span style=color:#c678dd>= </span><span>poly[::</span><span style=color:#c678dd>-</span><span style=color:#e5c07b>1</span><span>]
</span><span>    </span><span style=color:#61afef>print</span><span>(</span><span style=color:#c678dd>f</span><span style=color:#98c379>"p = </span><span>{poly[</span><span style=color:#e5c07b>0</span><span>]}</span><span style=color:#98c379>"</span><span>)
</span><span>    </span><span style=color:#c678dd>for </span><span>c </span><span style=color:#c678dd>in </span><span>poly[</span><span style=color:#e5c07b>1</span><span>:]:
</span><span>        </span><span style=color:#61afef>print</span><span>(</span><span style=color:#c678dd>f</span><span style=color:#98c379>"p = p * x + </span><span>{c}</span><span style=color:#98c379>"</span><span>)
</span><span>    </span><span style=color:#61afef>print</span><span>(</span><span style=color:#c678dd>f</span><span style=color:#98c379>"p = </span><span>{p}</span><span style=color:#98c379>"</span><span>)
</span><span>    </span><span style=color:#61afef>print</span><span>(</span><span style=color:#98c379>"error:"</span><span>, err </span><span style=color:#c678dd>/ </span><span>eps)
</span></code></pre><p>Conveniently, mpmath’s Chebyshev approximation also returns the maximum error, so I had something to compare with the benchmark results.<figure><img alt="Chebyshev approximation result" src=cheby-result.svg><figcaption>Comparison of Taylor and Chebyshev approximation</figcaption></figure><p>From the graph above, there are two things that needs attention:<ol><li><p>Chebyshev approximation is better, I indeed managed to reach the saturation point faster, at the cost of the quality of the saturation point.</p><li><p>The zigzag shape of the graph indicates that significant drops in error occurs when a new even-degree polynomial is reached.</p></ol><p>Remember that I’m optimizing for maximum error, not average error, so an approximation technique that distributes the error more evenly is advantageous. However, both problems above are the result of the unexploited symmetry of sine. An n-degree Chebyshev polynomial requires n - 1 Horner’s iteration, while for the truncated Taylor series, it’s n / 2 - 1. Less arithmetic operations mean less accumulating error, which explains why the saturated error of the Chebyshev approximation is so bad. Similarly, odd-degree polynomials don’t add much because the extra term that makes the function odd can’t capture the even features of the function in the input range.<h3 id=optimizing-chebyshev-approximation>Optimizing Chebyshev approximation</h3><p>So in order to optimize the Chebyshev polynomials, I had to make them either even or odd. If you have a general polynomial P(x), you can turn it into an even function by evaluating it at x^2 instead of x, giving P(x^2). So I generated Chebyshev approximations for sin(π√x) and evaluated it like the Taylor series.<p>But there’s a problem, sin(πx) is not even around 0, so the Chebyshev polynomials struggles to fit the function. To remedy this I had to shift the function by 0.5 similarly to the Taylor polynomials, so this is essentially a truncated Taylor series with a different set of coefficients. By shifting the function, we get the equality:<pre style=color:#dcdfe4;background-color:#282c34><code><span>P((x - 1/2)^2) = sin(πx)
</span><span>P(x) = sin(π√x + π/2) = cos(π√x)
</span></code></pre><p>It’s tempting to compute the Chebyshev coefficients to fit the range [0, 1], but we can see that:<pre style=color:#dcdfe4;background-color:#282c34><code><span>   0 <=      x      <= 1
</span><span>-1/2 <=   x - 1/2   <= 1/2
</span><span>   0 <= (x - 1/2)^2 <= 1/4
</span></code></pre><p>So I only had to fit the function to the range [0, 1/4]. The reduced range means that it’s easier to fit the function with less polynomial terms. This looks promising, let’s see how well it actually does:<figure><img alt="Imrpoved Chebyshev approximation result" src=cheby-improved-result.svg><figcaption>Comparison of Taylor and improved Chebyshev approximation</figcaption></figure><p>I finally made something better than the truncated Taylor series. The improved version reached the same saturation point as the Taylor polynomials, but it converged slightly faster. But something didn’t feel right. I computed sine by indirectly computing cosine. While there’s nothing wrong with that, it’s better if I can compute sine directly.<p>I had to convert to cosine because I need to transform sine into an even function, but there’s another sine-related even function: the sinc function or <code>sin(x)/x</code>.<pre style=color:#dcdfe4;background-color:#282c34><code><span>P(x^2) = sin(x)/x
</span><span>P(x) = sin(π√x)/√x
</span><span>sin(πx) = P(x^2) * x
</span></code></pre><p>Multiplying x by an even function creates an odd function, which is exactly what sine is. This feels more natural than the approach above and it also guarantees that sin(0)—and, by extension, sine of integers—zero. The problem with this method is that unlike the other method, there’s no way to compute the error in mpmath. As a result, I only compared the benchmark results of the two methods.<figure><img alt="Chebyshev approximation comparison" src=cheby-result-even-odd.svg><figcaption>Comparison between the two Chebyshev approximation methods</figcaption></figure><p>The odd-degree sinc transformation does slightly worse than the even-degree cos transformation. For the worse saturation point, it’s probably due to the extra multiplication, similar to the Padé approximation method. For the worse non-saturated error, however, I can hypothesize that it’s due to the additional complexity of approximating <code>sin(π√x)/√x</code> instead of <code>cos(π√x)</code>.<p>Nontheless, I prefer the odd-degree version because it felt more like directly computing sine and no matter how bad the approximation is, the sine of full turns is always zero.<h1 id=conclusion>Conclusion</h1><p>This post summarized the techniques I tried to compute sine, including lookup tables, truncated Taylor series, Padé approximant, and Chebyshev polynomials. These methods are nothing new, in fact, mpmath has implemented all these methods already, and all I did was call functions and evaluate the results. It was still pretty fun balancing precision, speed, and memory usage. I still want to learn more about Chebyshev polynomials and try Remez’s algorithm, which is pretty interesting as it uses Newton’s method, another approximation technique.<p>For the time being, this is what I use for computing sine in turns for single-precision floating-point numbers. It’s the odd-degree Chebyshev method with internal double conversion to bypass the saturation point. This has the maximum error of 0.5 ULP.<pre class=language-c data-lang=c style=color:#dcdfe4;background-color:#282c34><code class=language-c data-lang=c><span style=color:#c678dd>float </span><span style=color:#61afef>sin_turn_32</span><span>(</span><span style=color:#c678dd>float </span><span style=color:#e06c75>fx</span><span>) {
</span><span>  </span><span style=color:#c678dd>double</span><span> x </span><span style=color:#c678dd>=</span><span> fx </span><span style=color:#c678dd>* </span><span style=color:#e5c07b>2</span><span>;
</span><span>  int64_t ix </span><span style=color:#c678dd>= </span><span>(int64_t)(x </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.5</span><span>) </span><span style=color:#c678dd>- </span><span>(x </span><span style=color:#c678dd>< -</span><span style=color:#e5c07b>0.5</span><span>);
</span><span>  x </span><span style=color:#c678dd>-=</span><span> ix;
</span><span>  </span><span style=color:#c678dd>double</span><span> xx </span><span style=color:#c678dd>=</span><span> x </span><span style=color:#c678dd>*</span><span> x;
</span><span>  </span><span style=color:#c678dd>double</span><span> y </span><span style=color:#c678dd>= </span><span style=color:#e5c07b>0.077655912276013869412936155364</span><span>;
</span><span>  y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ -</span><span style=color:#e5c07b>0.59829041128342753490068232851</span><span>;
</span><span>  y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>2.5500773865289118653040643387</span><span>;
</span><span>  y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ -</span><span style=color:#e5c07b>5.167710076668314409260164949</span><span>;
</span><span>  y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>3.1415926400772032517452140952</span><span>;
</span><span>  </span><span style=color:#c678dd>if </span><span>(ix </span><span style=color:#c678dd>& </span><span style=color:#e5c07b>1</span><span>) y </span><span style=color:#c678dd>= -</span><span>y;
</span><span>  </span><span style=color:#c678dd>return</span><span> y </span><span style=color:#c678dd>*</span><span> x;
</span><span>}
</span></code></pre><p>For double-precision, more terms are needed to converge, and unfortunately, 80-bit and 128-bit floating-point numbers are too slow to do the previous optimization. I’m sure there are other clever ways of achieving this, but currently the precision has reached the saturation point at 2 ULP.<pre class=language-c data-lang=c style=color:#dcdfe4;background-color:#282c34><code class=language-c data-lang=c><span style=color:#c678dd>double </span><span style=color:#61afef>sin_turn_64</span><span>(</span><span style=color:#c678dd>double </span><span style=color:#e06c75>x</span><span>) {
</span><span>  x </span><span style=color:#c678dd>*= </span><span style=color:#e5c07b>2</span><span>;
</span><span>  int64_t ix </span><span style=color:#c678dd>= </span><span>(int64_t)(x </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.5</span><span>) </span><span style=color:#c678dd>- </span><span>(x </span><span style=color:#c678dd>< -</span><span style=color:#e5c07b>0.5</span><span>);
</span><span>  x </span><span style=color:#c678dd>-=</span><span> ix;
</span><span>  </span><span style=color:#c678dd>double</span><span> xx </span><span style=color:#c678dd>=</span><span> x </span><span style=color:#c678dd>*</span><span> x;
</span><span>  </span><span style=color:#c678dd>double</span><span> y </span><span style=color:#c678dd>= -</span><span style=color:#e5c07b>0.000021133627352052970404406818407</span><span>;
</span><span>  y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.00046598701580433779333641383896</span><span>;
</span><span>  y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ -</span><span style=color:#e5c07b>0.0073703643265304156755464627222</span><span>;
</span><span>  y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.082145878816310426916561014931</span><span>;
</span><span>  y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ -</span><span style=color:#e5c07b>0.59926452882522400507990958357</span><span>;
</span><span>  y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>2.5501640398618676296938350463</span><span>;
</span><span>  y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ -</span><span style=color:#e5c07b>5.167712780049785822740496262</span><span>;
</span><span>  y </span><span style=color:#c678dd>=</span><span> y </span><span style=color:#c678dd>*</span><span> xx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>3.1415926535897928787046988706</span><span>;
</span><span>  </span><span style=color:#c678dd>if </span><span>(ix </span><span style=color:#c678dd>& </span><span style=color:#e5c07b>1</span><span>) y </span><span style=color:#c678dd>= -</span><span>y;
</span><span>  </span><span style=color:#c678dd>return</span><span> y </span><span style=color:#c678dd>*</span><span> x;
</span><span>}
</span></code></pre><p>It’s interesting to benchmark and compare these with the standard library implementation, but I don’t expect it to do better. The benefit of creating my own sine function is that I now have full control over it and can change it however I want. If one day I decide that half-turns or degrees are better, then I can easily switch to those. I can also use a lower-degree polynomial to increase execution speed at the cost of precision. It’s always nice to have the ability to customize a tool for your needs and preferences.<h1 id=uupdate-benchmark-against-the-standard-library-implementation>Uupdate: Benchmark against the standard library implementation</h1><p>I couldn’t help myself but to conduct some benchmarks to compare my custom sine implementation with the implementation from the standard library. I’ll test both computing a single value and batch processing an entire array of values. To generate inputs, I used the <a href=//prng.di.unimi.it/>Xoshiro256++</a> random number generators to generate 64-bit values in these value ranges: [-0.25, 0.25) and [-1, 1) I also tested a SIMD optimized implementation of <a href="//en.wikipedia.org/wiki/Estrin's_scheme">Estrin’s scheme</a>.<pre class=language-c data-lang=c style=color:#dcdfe4;background-color:#282c34><code class=language-c data-lang=c><span style=color:#c678dd>double </span><span style=color:#61afef>sin_turn_simd</span><span>(</span><span style=color:#c678dd>double </span><span style=color:#e06c75>x</span><span>) {
</span><span>    x </span><span style=color:#c678dd>*= </span><span style=color:#e5c07b>2</span><span>;
</span><span>    int64_t ix </span><span style=color:#c678dd>= </span><span>(int64_t)(x </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0.5</span><span>) </span><span style=color:#c678dd>- </span><span>(x </span><span style=color:#c678dd>< -</span><span style=color:#e5c07b>0.5</span><span>);
</span><span>    x </span><span style=color:#c678dd>-=</span><span> ix;
</span><span>    </span><span style=color:#c678dd>double</span><span> x2 </span><span style=color:#c678dd>=</span><span> x </span><span style=color:#c678dd>*</span><span> x;
</span><span>    </span><span style=color:#c678dd>double</span><span> x4 </span><span style=color:#c678dd>=</span><span> x2 </span><span style=color:#c678dd>*</span><span> x2;
</span><span>    </span><span style=color:#c678dd>double</span><span> x8 </span><span style=color:#c678dd>=</span><span> x4 </span><span style=color:#c678dd>*</span><span> x4;
</span><span>
</span><span>    __m256d c0 </span><span style=color:#c678dd>= </span><span style=color:#e06c75>_mm256_set_pd</span><span>(
</span><span>        </span><span style=color:#c678dd>-</span><span style=color:#e5c07b>5.167712780049785822740496262</span><span>,
</span><span>        </span><span style=color:#c678dd>-</span><span style=color:#e5c07b>0.0073703643265304156755464627222</span><span>,
</span><span>        </span><span style=color:#c678dd>-</span><span style=color:#e5c07b>0.59926452882522400507990958357</span><span>,
</span><span>        </span><span style=color:#c678dd>-</span><span style=color:#e5c07b>0.000021133627352052970404406818407
</span><span>    )
</span><span>
</span><span>    __m256d c1 </span><span style=color:#c678dd>= </span><span style=color:#e06c75>_mm256_set_pd</span><span>(
</span><span>        </span><span style=color:#e5c07b>3.1415926535897928787046988706</span><span>,
</span><span>        </span><span style=color:#e5c07b>0.082145878816310426916561014931</span><span>,
</span><span>        </span><span style=color:#e5c07b>2.5501640398618676296938350463</span><span>,
</span><span>        </span><span style=color:#e5c07b>0.00046598701580433779333641383896
</span><span>    )
</span><span>
</span><span>    __m256d p0 </span><span style=color:#c678dd>= </span><span style=color:#e06c75>_mm256_fmadd_pd</span><span>(c0, x2, c1);
</span><span>    __m128d p1 </span><span style=color:#c678dd>= </span><span style=color:#e06c75>_mm_fmadd_pd</span><span>(</span><span style=color:#e06c75>_mm256_extractf128_pd</span><span>(p0, </span><span style=color:#e5c07b>0</span><span>), </span><span style=color:#e06c75>_mm_set1_pd</span><span>(x4), </span><span style=color:#e06c75>_mm256_extractf128_pd</span><span>(p0, </span><span style=color:#e5c07b>1</span><span>));
</span><span>    </span><span style=color:#c678dd>double</span><span> p2[</span><span style=color:#e5c07b>2</span><span>];
</span><span>    </span><span style=color:#e06c75>_mm_store_pd</span><span>(p2, p1);
</span><span>
</span><span>    </span><span style=color:#c678dd>double</span><span> y </span><span style=color:#c678dd>=</span><span> p2[</span><span style=color:#e5c07b>0</span><span>] </span><span style=color:#c678dd>*</span><span> x8 </span><span style=color:#c678dd>+</span><span> p2[</span><span style=color:#e5c07b>1</span><span>];
</span><span>    </span><span style=color:#c678dd>if </span><span>(ix </span><span style=color:#c678dd>& </span><span style=color:#e5c07b>1</span><span>) y </span><span style=color:#c678dd>= -</span><span>y;
</span><span>    </span><span style=color:#c678dd>return</span><span> y </span><span style=color:#c678dd>*</span><span> x;
</span><span>}
</span></code></pre><p>To evaluate the speed of these algorithms, I measured the time it took to compute 100 million sines. The error is measured against a degree-41 Chebyshev polynomial operating in 80-bit floating-point.<h2 id=single-processing-results>Single processing results</h2><p>Input range [-0.25, 0.25):<table><thead><tr><th><th>GCC <code>-O3</code><th>GCC <code>-Ofast</code><th>Clang <code>-O3</code><th>Clang <code>-Ofast</code><th>Error<tbody><tr><td>Horner<td>596 ms<td>616 ms<td>593 ms<td>539 ms<td>2 ULP<tr><td>SIMD<td><strong>587 ms</strong><td>616 ms<td><strong>584 ms</strong><td><strong>535 ms</strong><td>2.5 ULP<tr><td>Glibc<td>1382 ms<td>1394 ms<td>1366 ms<td>1247 ms<td><strong>0.5 ULP</strong></table><p>Input range [-1, 1):<table><thead><tr><th><th>GCC <code>-O3</code><th>GCC <code>-Ofast</code><th>Clang <code>-O3</code><th>Clang <code>-Ofast</code><th>Error<tbody><tr><td>Horner<td>1131 ms<td>1090 ms<td>1213 ms<td>1108 ms<td><strong>2 ULP</strong><tr><td>SIMD<td><strong>1102 ms</strong><td><strong>1086 ms</strong><td><strong>1205 ms</strong><td><strong>1101 ms</strong><td>2.5 ULP<tr><td>Glibc<td>1936 ms<td>1937 ms<td>1920 ms<td>1819 ms<td>3.125 ULP</table><h2 id=batch-processing-results>Batch processing results</h2><p>Input range [-0.25, 0.25):<table><thead><tr><th><th>GCC <code>-O3</code><th>GCC <code>-Ofast</code><th>Clang <code>-O3</code><th>Clang <code>-Ofast</code><th>Error<tbody><tr><td>Horner<td><strong>488 ms</strong><td>492 ms<td><strong>461 ms</strong><td><strong>458 ms</strong><td>2 ULP<tr><td>SIMD<td>503 ms<td>503 ms<td>505 ms<td>504 ms<td>2.5 ULP<tr><td>Glibc<td>1172 ms<td><strong>388 ms</strong> (1.5 ULP)<td>1137 ms<td>1247 ms<td><strong>0.5 ULP</strong></table><p>Input range [-1, 1):<table><thead><tr><th><th>GCC <code>-O3</code><th>GCC <code>-Ofast</code><th>Clang <code>-O3</code><th>Clang <code>-Ofast</code><th>Error<tbody><tr><td>Horner<td><strong>930 ms</strong><td>919 ms<td><strong>469 ms</strong><td><strong>458 ms</strong><td><strong>2 ULP</strong><tr><td>SIMD<td>942 ms<td>913 ms<td>944 ms<td>898 ms<td>2.5 ULP<tr><td>Glibc<td>1753 ms<td><strong>395 ms</strong> (3.25 ULP)<td>1743 ms<td>1718 ms<td>3.125 ULP</table><h2 id=analysis>Analysis</h2><p>Although performance wasn’t my primary goal when creating a sine approximation, it’s pleasant to see that my custom implementation is faster than the glibc implementation. There are many reasons why this is the case:<ul><li>The glibc implementation has proper infinity and NaN handling.<li>The glibc implementation performs more aggressive argument reduction and exploits even more symmetry (hence the 0.5 ULP).<li>My implementation has a simpler and faster argument reduction procedure.<li>My implementation is smaller, possibly making it easier to inline and optimize.</ul><p>For single processing speed, the SIMD version consistently outperforms the Horner’s method implementation, but only in an insignificant amount. For the batch processing, however, the SIMD version performs worse due to the fact that both versions are vectorized anyway and the SIMD version has some extra overheads. The biggest drawback of the SIMD version is that Estrin’s scheme isn’t optimal in terms of the number of operations, so it has more error accumulation. While the idea is cool and was also pretty fun to implement, the SIMD version isn’t worth it.<p>My implementations only perform worse than the GCC <code>-Ofast</code> optimized glibc sine in bulk processing. A quick look at the generated assembly revealed that it directly calls <code>_ZGVdN4v_sin</code>, which is a carefully optimized SIMD implementation of sine in <a href=//sourceware.org/glibc/wiki/libmvec>libmvec</a>. Overall, I was pleasantly surprised that my implementation is quite efficient and accurate, and it made me even happier with what I managed to come up with.</article></main><footer class=footer__container><svg viewbox="0 352.5 960 188.5" class=footer__split preserveaspectratio=none version=1.1 xmlns=http://www.w3.org/2000/svg xmlns:xlink=http://www.w3.org/1999/xlink><path d="M0 441L22.8 424.5C45.7 408 91.3 375 137 360.8C182.7 346.7 228.3 351.3 274 369.3C319.7 387.3 365.3 418.7 411.2 414.2C457 409.7 503 369.3 548.8 368C594.7 366.7 640.3 404.3 686 421.7C731.7 439 777.3 436 823 429.8C868.7 423.7 914.3 414.3 937.2 409.7L960 405L960 541L937.2 541C914.3 541 868.7 541 823 541C777.3 541 731.7 541 686 541C640.3 541 594.7 541 548.8 541C503 541 457 541 411.2 541C365.3 541 319.7 541 274 541C228.3 541 182.7 541 137 541C91.3 541 45.7 541 22.8 541L0 541Z" fill=#f7f9fb /><path d="M0 477L22.8 467C45.7 457 91.3 437 137 423.8C182.7 410.7 228.3 404.3 274 401.7C319.7 399 365.3 400 411.2 413.5C457 427 503 453 548.8 464.2C594.7 475.3 640.3 471.7 686 456.5C731.7 441.3 777.3 414.7 823 404.5C868.7 394.3 914.3 400.7 937.2 403.8L960 407L960 541L937.2 541C914.3 541 868.7 541 823 541C777.3 541 731.7 541 686 541C640.3 541 594.7 541 548.8 541C503 541 457 541 411.2 541C365.3 541 319.7 541 274 541C228.3 541 182.7 541 137 541C91.3 541 45.7 541 22.8 541L0 541Z" fill=#eff3f7 /><path d="M0 459L22.8 456.2C45.7 453.3 91.3 447.7 137 456.7C182.7 465.7 228.3 489.3 274 494.8C319.7 500.3 365.3 487.7 411.2 476C457 464.3 503 453.7 548.8 451.7C594.7 449.7 640.3 456.3 686 462.5C731.7 468.7 777.3 474.3 823 473.2C868.7 472 914.3 464 937.2 460L960 456L960 541L937.2 541C914.3 541 868.7 541 823 541C777.3 541 731.7 541 686 541C640.3 541 594.7 541 548.8 541C503 541 457 541 411.2 541C365.3 541 319.7 541 274 541C228.3 541 182.7 541 137 541C91.3 541 45.7 541 22.8 541L0 541Z" fill=#e8eef4 stroke=#e8eef4 /></svg><div class=footer__bg><div class=footer>© 2024 Zap (Huy-Giap Bui). Content on this site is licensed under <a href=//creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0</a>.</div></div></footer></div>