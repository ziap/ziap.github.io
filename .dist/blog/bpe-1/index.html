<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="Bui Huy Giap's personal website and blog" name=description><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon><title>
      
        
          Fast BPE tokenizer - Overview and arena allocated decoding | Zap's website
        
      
    </title><link href=/styles/fonts.css rel=stylesheet><style>h1{font-size:1.682rem}h2{font-size:1.414rem}h3{font-size:1.189rem}h4{font-size:1rem}h5{font-size:.841rem}h6{font-size:.707rem}</style><link href=/styles/base.css rel=stylesheet><link href=/styles/nav.css rel=stylesheet><link href=/styles/footer.css rel=stylesheet><link href=/rss.xml rel=alternate title=RSS type=application/rss+xml><link href=/styles/page.css rel=stylesheet><body><div class=nav__bg><div class=nav__container><nav class=nav><h3 class=nav__title><a class=nav__title__link href=/>Zap</a></h3><ul class=nav__links><li><a class=nav__link href=/works>Works</a><li><a class=nav__link href=/blog>Blog</a><li><a class=nav__link href=/about>About</a></ul></nav></div></div><div class=container><main class=content><header class=post-header><h1 class="post-header__title title">Fast BPE tokenizer - Overview and arena allocated decoding</h1><div class=post-header__meta><div class=post-header__data><svg viewbox="0 0 448 512" height=1em xmlns=http://www.w3.org/2000/svg><path d="M128 0c17.7 0 32 14.3 32 32V64H288V32c0-17.7 14.3-32 32-32s32 14.3 32 32V64h48c26.5 0 48 21.5 48 48v48H0V112C0 85.5 21.5 64 48 64H96V32c0-17.7 14.3-32 32-32zM0 192H448V464c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V192zm64 80v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V272c0-8.8-7.2-16-16-16H80c-8.8 0-16 7.2-16 16zm128 0v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V272c0-8.8-7.2-16-16-16H208c-8.8 0-16 7.2-16 16zm144-16c-8.8 0-16 7.2-16 16v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V272c0-8.8-7.2-16-16-16H336zM64 400v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V400c0-8.8-7.2-16-16-16H80c-8.8 0-16 7.2-16 16zm144-16c-8.8 0-16 7.2-16 16v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V400c0-8.8-7.2-16-16-16H208zm112 16v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V400c0-8.8-7.2-16-16-16H336c-8.8 0-16 7.2-16 16z"/></svg> Wed, Sep 24 2025</div><div class=post-header__data><svg viewbox="0 0 512 512" height=1em xmlns=http://www.w3.org/2000/svg><path d="M256 0a256 256 0 1 1 0 512A256 256 0 1 1 256 0zM232 120V256c0 8 4 15.5 10.7 20l96 64c11 7.4 25.9 4.4 33.3-6.7s4.4-25.9-6.7-33.3L280 243.2V120c0-13.3-10.7-24-24-24s-24 10.7-24 24z"/></svg><abbr title="1688 words"> 7 minutes read </abbr></div></div></header><article class=content__body><p>This is the first entry of a series of articles about designing an efficient byte pair encoder (BPE) tokenizer. The tokenized text is then used to train an n-gram model for the task of synthesizing placeholder text. These articles are for sharing the design decisions and optimization techniques I applied. For the first entry, I wanted to talk about an overview of the algorithm and an efficient decoder implementation.</p><span id=continue-reading></span><h1 id=overview-of-the-bpe-algorithm>Overview of the BPE Algorithm</h1><p>BPE originally started as a text compression algorithm. But in order to losslessly compress any data, you need to “learn” and exploit redundancies in the dataset. It turns out that this particular compression technique learns patterns and grammars so well that running machine learning models on the compressed text helps with convergence and accuracy. This is why BPE tokenization is a crucial component of modern LLMs.<p>For my implementation, this algorithm is separated into two parts:<ul><li><strong>Encoder:</strong> trains on the input text and output a list of tokens and their corresponding pairs.<li><strong>Decoder:</strong> convert tokens back into text.</ul><h2 id=bpe-decoding>BPE Decoding</h2><p>Just like most compression algorithms, decoding is relatively simpler and faster. In fact, unlike the encoder, this article will fully cover the process of implementing an optimized BPE decoder. I’ll also start explaining the decoder first. For the example, we will use the first two sentences from the “Lorem Ipsum” placeholder text.<blockquote><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.<p>Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.</blockquote><p>Assuming that after encoding, we have the following table:<div style=justify-content:space-between;gap:1rem;display:flex><table><thead><tr><th>Token<th>Left<th>Right<tbody><tr><td>256<td>32<td>101<tr><td>257<td>111<td>114<tr><td>258<td>32<td>97<tr><td>259<td>110<td>105<tr><td>260<td>113<td>117<tr><td>261<td>99<td>111<tr><td>262<td>100<td>111<tr><td>263<td>32<td>262<tr><td>264<td>32<td>261<tr><td>265<td>257<td>101<tr><td>266<td>108<td>97<tr><td>267<td>32<td>117</table><table><thead><tr><th>Token<th>Left<th>Right<tbody><tr><td>268<td>108<td>105<tr><td>269<td>99<td>105<tr><td>270<td>105<td>115<tr><td>271<td>115<td>101<tr><td>272<td>105<td>112<tr><td>273<td>110<td>271<tr><td>274<td>264<td>273<tr><td>275<td>270<td>32<tr><td>276<td>268<td>260<tr><td>277<td>258<td>276<tr><td>278<td>267<td>116<tr><td>279<td>266<td>98</table><table><thead><tr><th>Token<th>Left<th>Right<tbody><tr><td>280<td>32<td>279<tr><td>281<td>263<td>108<tr><td>282<td>259<td>109<tr><td>283<td>258<td>100<tr><td>284<td>257<td>32<tr><td>285<td>256<td>120<tr><td>286<td>97<td>116<tr><td>287<td>32<td>109<tr><td>288<td>109<td>111<tr><td>289<td>116<td>101<tr><td>290<td>44<td>32<tr><td>291<td>115<td>105</table></div><p>Note that the tokens start at <code>256</code> since <code>0</code> to <code>255</code> are reserved for single-byte characters. So, how do you decode the token <code>274</code>. Looking up the table, the token <code>274</code> consists of the token <code>264</code> on the left and the token <code>273</code> on the right. So we expand into them. We do this recursively until each token is a single-byte token or is less than <code>256</code>.<pre style=color:#dcdfe4;background-color:#282c34><code><span>274 => (>264, 273<)
</span><span>    => (>32, 261<, >110, 271<)
</span><span>    => (32, >99, 111<, 110, >115, 101<)
</span><span>    => " conse"
</span></code></pre><p>Note that this pattern appears twice in the input text (<code>amet,| conse|ctetur</code> and <code>commodo| conse|quat</code>). This example is only for demonstrating BPE decoding. A larger example is required to comment on the behavior of the algorithm and the characteristics of the generated tokens.<h2 id=bpe-encoding>BPE Encoding</h2><p>The algorithm for generating tokens from an input text proceeds as follow:<ul><li>While not enough tokens generated: <ul><li>Select the pair that occurs the most in the input text.<li>If that pair occurs only once, stop.<li>Create and store a new token from the pair.<li>Replace all instances of that pair with the newly created token.</ul></ul><p>Because the algorithm is a bit more complicated, we’ll use a smaller example from the <a href=//huggingface.co/learn/llm-course/chapter6/5>huggingface LLM course</a>, just <code>hug pug pun bun hugs</code>. This example uses lowercase characters as single-byte characters, and uppercase characters as the generated tokens.<p>First, enumerate over all pairs in the text:<pre style=color:#dcdfe4;background-color:#282c34><code><span>hu ug g_ _p pu ug g_ _p pu un n_ _b bu un n_ _h hu ug gs
</span></code></pre><p>Then, collects unique pairs and count their occurences:<pre style=color:#dcdfe4;background-color:#282c34><code><span>hu => 2
</span><span>ug => 3
</span><span>g_ => 2
</span><span>_p => 2
</span><span>pu => 2
</span><span>un => 2
</span><span>n_ => 2
</span><span>_b => 1
</span><span>bu => 1
</span><span>_h => 1
</span><span>gs => 1
</span></code></pre><p>The pair that occurs the most is <code>ug</code>, so we’ll create a new token <code>A = (u, g)</code> and substitute all instances of <code>ug</code> with <code>A</code>.<pre style=color:#dcdfe4;background-color:#282c34><code><span>A = (u, g)
</span><span>
</span><span>hA pA pun bun hAs
</span></code></pre><p>Repeat this until we have generated enough tokens or all pairs in the final text are unique.<pre style=color:#dcdfe4;background-color:#282c34><code><span>text: "hug pug pun bun hugs"
</span><span>
</span><span>iteration: 1
</span><span>new_token: A => (u, g)
</span><span>text: "hA pA pun bun hAs"
</span><span>
</span><span>iteration: 2
</span><span>new_token: B => (h, A)
</span><span>text: "B pA pun bun Bs"
</span><span>
</span><span>iteration: 3
</span><span>new_token: C => (_, p)
</span><span>text: "BCACun bun Bs"
</span><span>
</span><span>iteration: 4
</span><span>new_token: D => (u, n)
</span><span>text: "BCACD bD Bs"
</span><span>
</span><span>iteration: 5
</span><span>new_token: E => (D, _)
</span><span>text: "BCACEbEBs"
</span><span>
</span><span>final_tokens:
</span><span>A => (u, g)
</span><span>B => (h, A)
</span><span>C => (_, p)
</span><span>D => (u, n)
</span><span>E => (D, _)
</span><span>
</span><span>tokenized_text: "BCACEbEBs"
</span></code></pre><p>These tokens can then be decoded using the process above, and if you decode the tokenized text sequence, you get back the original text. Therefore, we can say that this encoding is lossless.<h1 id=implementation>Implementation</h1><p>I implemented the algorithm in Rust because the algorithm is quite complex and I want the extra bits of correctness aid that Rust provides. I was also not trying to squeeze every bit of performance. Nor will I try to compete with production-ready solutions such as <a href=//github.com/openai/tiktoken>tiktoken</a> or <a href=//github.com/huggingface/tokenizers>the huggingface tokenizer</a>. As such, the implementation will not be optimized with SIMD, multi-threading, and aggressive bound check removal. Here are the goals of the implementation:<ul><li><strong>Correctness:</strong> Obviously, this also means minimizing <code>unsafe</code> code.<li><strong>Optimal time complexity:</strong> This means implementing complex, finicky data structures to achieve <code>O(n)</code> training, where <code>n</code> is the input length.<li><strong>Reasonable data layout:</strong> Applies principles of data-oriented design, structuring data based on how they are accessed and manipulated. Avoid small, decentralized allocations and strive for compact, fast (de)serialization.</ul><p>I also hardcoded the target tokens to <code>65536 - 256 = 65280</code> tokens such that we can represent them as 16-bit unsigned integers. The encoder will run until all token pairs in the tokenized text are unique, or we have exhausted 16-bits of tokens. GPT2 has <a href=//huggingface.co/docs/transformers/model_doc/gpt2>50257</a> tokens, so there’s enough wiggle room for even an early LLM model.<h2 id=decoding-and-token-representation>Decoding and Token Representation</h2><p>Let’s start with the decoder. We only have to think about how to decode a single token, as the most efficient way to decode a sequence of tokens is to sequentially decode each individual token. I also defined the input of the decoder to be a simple list of pairs, and the index into the list denotes the associated token. For example, the token table from above:<table><thead><tr><th>Token<th>Left<th>Right<tbody><tr><td>256<td>32<td>101<tr><td>257<td>111<td>114<tr><td>258<td>32<td>97<tr><td>259<td>110<td>105<tr><td>260<td>113<td>117<tr><td>261<td>99<td>111<tr><td>262<td>100<td>111<tr><td>263<td>32<td>262<tr><td>264<td>32<td>261<tr><td>265<td>257<td>101<tr><td>266<td>108<td>97<tr><td>267<td>32<td>117</table><p>Are represented as:<pre class=language-rs data-lang=rs style=color:#dcdfe4;background-color:#282c34><code class=language-rs data-lang=rs><span style=color:#c678dd>let</span><span> pairs: [(</span><span style=color:#c678dd>u16</span><span>, </span><span style=color:#c678dd>u16</span><span>)] </span><span style=color:#c678dd>= &</span><span>[
</span><span>  (</span><span style=color:#e5c07b>32</span><span>, </span><span style=color:#e5c07b>101</span><span>), (</span><span style=color:#e5c07b>111</span><span>, </span><span style=color:#e5c07b>114</span><span>), (</span><span style=color:#e5c07b>32</span><span>, </span><span style=color:#e5c07b>97</span><span>), (</span><span style=color:#e5c07b>110</span><span>, </span><span style=color:#e5c07b>105</span><span>), (</span><span style=color:#e5c07b>113</span><span>, </span><span style=color:#e5c07b>117</span><span>), (</span><span style=color:#e5c07b>99</span><span>, </span><span style=color:#e5c07b>111</span><span>),
</span><span>  (</span><span style=color:#e5c07b>100</span><span>, </span><span style=color:#e5c07b>111</span><span>), (</span><span style=color:#e5c07b>32</span><span>, </span><span style=color:#e5c07b>262</span><span>), (</span><span style=color:#e5c07b>32</span><span>, </span><span style=color:#e5c07b>261</span><span>), (</span><span style=color:#e5c07b>257</span><span>, </span><span style=color:#e5c07b>101</span><span>), (</span><span style=color:#e5c07b>108</span><span>, </span><span style=color:#e5c07b>97</span><span>), (</span><span style=color:#e5c07b>32</span><span>, </span><span style=color:#e5c07b>117</span><span>),
</span><span>];
</span></code></pre><p>To get, for example, token <code>262</code>, we first subtract by the smallest non-singe-byte token, which is <code>256</code>, and index the array, or <code>pairs[262 - 256]</code>. The optimal time complexity decoding algorithm can be implemented recursively as described above:<pre class=language-rs data-lang=rs style=color:#dcdfe4;background-color:#282c34><code class=language-rs data-lang=rs><span style=color:#c678dd>struct </span><span>Decoder {
</span><span>  </span><span style=color:#e06c75>pairs</span><span>: Box<[(</span><span style=color:#c678dd>u16</span><span>, </span><span style=color:#c678dd>u16</span><span>)]>,
</span><span>}
</span><span>
</span><span style=color:#c678dd>impl </span><span>Decoder {
</span><span>  </span><span style=color:#c678dd>fn </span><span style=color:#61afef>decode</span><span>(</span><span style=color:#c678dd>&</span><span style=color:#e06c75>self</span><span>, </span><span style=color:#e06c75>token</span><span>: </span><span style=color:#c678dd>u16</span><span>, </span><span style=color:#e06c75>out</span><span>: </span><span style=color:#c678dd>&mut</span><span> impl Write) -> std::io::Result<()> {
</span><span>    </span><span style=color:#c678dd>if</span><span> token </span><span style=color:#c678dd>< </span><span style=color:#e5c07b>256 </span><span>{
</span><span>      out.</span><span style=color:#61afef>write</span><span>(</span><span style=color:#c678dd>&</span><span>[token </span><span style=color:#c678dd>as u8</span><span>])</span><span style=color:#c678dd>?</span><span>;
</span><span>    } </span><span style=color:#c678dd>else </span><span>{
</span><span>      </span><span style=color:#c678dd>let </span><span>(left, right) </span><span style=color:#c678dd>= </span><span style=color:#e06c75>self</span><span>.pairs[token </span><span style=color:#c678dd>as usize - </span><span style=color:#e5c07b>256</span><span>];
</span><span>      </span><span style=color:#e06c75>self</span><span>.</span><span style=color:#61afef>decode</span><span>(left, out)</span><span style=color:#c678dd>?</span><span>;
</span><span>      </span><span style=color:#e06c75>self</span><span>.</span><span style=color:#61afef>decode</span><span>(right, out)</span><span style=color:#c678dd>?</span><span>;
</span><span>    }
</span><span>
</span><span>    Ok(())
</span><span>  }
</span><span>}
</span></code></pre><p>This function takes a writer as its output, so you can output to a <code>Vec&LTu8></code><pre class=language-rs data-lang=rs style=color:#dcdfe4;background-color:#282c34><code class=language-rs data-lang=rs><span style=color:#c678dd>fn </span><span style=color:#61afef>decode_all</span><span>(</span><span style=color:#e06c75>decoder</span><span>: </span><span style=color:#c678dd>&</span><span>Decoder, </span><span style=color:#e06c75>tokens</span><span>: </span><span style=color:#c678dd>&</span><span>[</span><span style=color:#c678dd>u16</span><span>]) -> Vec<</span><span style=color:#c678dd>u8</span><span>> {
</span><span>  </span><span style=color:#c678dd>let mut</span><span> buf: Vec<</span><span style=color:#c678dd>u8</span><span>> </span><span style=color:#c678dd>= </span><span>Vec::new();
</span><span>
</span><span>  </span><span style=color:#c678dd>for</span><span> token </span><span style=color:#c678dd>in</span><span> tokens {
</span><span>    decoder.</span><span style=color:#61afef>decode</span><span>(token, </span><span style=color:#c678dd>&mut</span><span> buf).</span><span style=color:#61afef>unwrap</span><span>();
</span><span>  }
</span><span>
</span><span>  buf
</span><span>}
</span></code></pre><p>Or directly into a stream such as <code>stdout</code>. But there are two main problems: it’s a recursive function, and the memory access pattern is horrible. Let’s use the example from above.<pre style=color:#dcdfe4;background-color:#282c34><code><span>274 => (>264, 273<)
</span><span>    => (>32, 261<, >110, 271<)
</span><span>    => (32, >99, 111<, 110, >115, 101<)
</span><span>    => " conse"
</span></code></pre><p>Running the recursive algorithm will result in an access pattern as follows:<pre style=color:#dcdfe4;background-color:#282c34><code><span>18 -> 8 -> 5 -> 17 -> 15
</span></code></pre><p>The pattern is random and has no spatial locality. This is not very cache-friendly, especially when we scale up the input data size and number of tokens. What if, instead, we eagerly precompute the decoded value of all tokens, and decoding a token is just a simple table lookup?<pre class=language-rs data-lang=rs style=color:#dcdfe4;background-color:#282c34><code class=language-rs data-lang=rs><span style=color:#c678dd>struct </span><span>Decoder {
</span><span>  </span><span style=color:#e06c75>data</span><span>: Box<[Box<[</span><span style=color:#c678dd>u8</span><span>]>]>,
</span><span>}
</span><span>
</span><span style=color:#c678dd>impl </span><span>Decoder {
</span><span>  </span><span style=color:#c678dd>fn </span><span style=color:#61afef>new</span><span>(</span><span style=color:#e06c75>pairs</span><span>: </span><span style=color:#c678dd>&</span><span>[(</span><span style=color:#e06c75>u16</span><span>, </span><span style=color:#e06c75>u16</span><span>)]) -> </span><span style=color:#c678dd>Self </span><span>{
</span><span>    unimplemented!();
</span><span>  }
</span><span>
</span><span>  </span><span style=color:#c678dd>fn </span><span style=color:#61afef>decode</span><span>(</span><span style=color:#c678dd>&</span><span style=color:#e06c75>self</span><span>, </span><span style=color:#e06c75>token</span><span>: </span><span style=color:#c678dd>u16</span><span>) -> </span><span style=color:#c678dd>&</span><span>[</span><span style=color:#c678dd>u8</span><span>] {
</span><span>    </span><span style=color:#c678dd>&*</span><span style=color:#e06c75>self</span><span>.data[token </span><span style=color:#c678dd>as usize</span><span>]
</span><span>  }
</span><span>}
</span></code></pre><p>It’s just a simple array lookup, branchless (except for the bound check), and gives you more flexibility. Previously you needed to provide a sink to write the data to. Now you have direct access to the slice and can do whatever you want with it. The caveat is that we now use more memory, but that was a trade-off that I was willing to accept.<p>Now the problem becomes how to fill up the <code>data</code> array. Again, the optimal algorithm in terms of time complexity is to iterate over all tokens and call the recursive algorithm since we have to construct byte by byte anyway. However, if you need to compute many values of a recursive sequence, <a href=//cp-algorithms.com/dynamic_programming/intro-to-dp.html>dynamic programming</a> comes to mind.<pre class=language-rs data-lang=rs style=color:#dcdfe4;background-color:#282c34><code class=language-rs data-lang=rs><span style=color:#c678dd>struct </span><span>Decoder {
</span><span>  </span><span style=color:#e06c75>data</span><span>: Box<[Box<[</span><span style=color:#c678dd>u8</span><span>]>]>,
</span><span>}
</span><span>
</span><span style=color:#c678dd>impl </span><span>Decoder {
</span><span>  </span><span style=color:#c678dd>fn </span><span style=color:#61afef>new</span><span>(</span><span style=color:#e06c75>pairs</span><span>: </span><span style=color:#c678dd>&</span><span>[(</span><span style=color:#e06c75>u16</span><span>, </span><span style=color:#e06c75>u16</span><span>)]) -> </span><span style=color:#c678dd>Self </span><span>{
</span><span>    </span><span style=color:#c678dd>let mut</span><span> dp: Vec&LTBox<[</span><span style=color:#c678dd>u8</span><span>]>> </span><span style=color:#c678dd>= </span><span>(</span><span style=color:#e5c07b>0</span><span style=color:#c678dd>..=</span><span style=color:#e5c07b>255</span><span>).</span><span style=color:#61afef>map</span><span>(|</span><span style=color:#e06c75>x</span><span>| [x].</span><span style=color:#61afef>into</span><span>()).</span><span style=color:#61afef>collect</span><span>();
</span><span>    dp.</span><span style=color:#61afef>reserve</span><span>(pairs.</span><span style=color:#61afef>len</span><span>());
</span><span>
</span><span>    </span><span style=color:#c678dd>for &</span><span>(left, right) </span><span style=color:#c678dd>in</span><span> pairs {
</span><span>      </span><span style=color:#c678dd>let mut</span><span> buf: Vec<</span><span style=color:#c678dd>u8</span><span>> </span><span style=color:#c678dd>= </span><span>Vec::new();
</span><span>      buf.</span><span style=color:#61afef>extend_from_slice</span><span>(</span><span style=color:#c678dd>&*</span><span>dp[left </span><span style=color:#c678dd>as usize</span><span>]);
</span><span>      buf.</span><span style=color:#61afef>extend_from_slice</span><span>(</span><span style=color:#c678dd>&*</span><span>dp[right </span><span style=color:#c678dd>as usize</span><span>]);
</span><span>      dp.</span><span style=color:#61afef>push</span><span>(buf.</span><span style=color:#61afef>into</span><span>());
</span><span>    }
</span><span>    
</span><span>    </span><span style=color:#c678dd>Self </span><span>{ data: dp.</span><span style=color:#61afef>into</span><span>() }
</span><span>  }
</span><span>
</span><span>  </span><span style=color:#c678dd>fn </span><span style=color:#61afef>decode</span><span>(</span><span style=color:#c678dd>&</span><span style=color:#e06c75>self</span><span>, </span><span style=color:#e06c75>token</span><span>: </span><span style=color:#c678dd>u16</span><span>) -> </span><span style=color:#c678dd>&</span><span>[</span><span style=color:#c678dd>u8</span><span>] {
</span><span>    </span><span style=color:#c678dd>&*</span><span style=color:#e06c75>self</span><span>.data[token </span><span style=color:#c678dd>as usize</span><span>]
</span><span>  }
</span><span>}
</span></code></pre><p>This is much better already, as it effectively eliminated the recursion. However, we used a <a href=//en.wikipedia.org/wiki/Jagged_array>jagged array</a> which is usually not good. Each token is potentially small (about 5 characters), so this creates a lot of small, fragmented heap allocations. As usual, this is also bad for cache locality and puts extra work on the memory allocator. We can fix this using one of my favorite techniques: <a href=/blog/arena>arena allocation</a>.<p>In this approach, all of the data is stored in a contiguous buffer (or arena), and the lookup table can be implemented as an array of slices into the buffer. Each element is now a numeric offset, and the length can be inferred from the next offset.<pre class=language-rs data-lang=rs style=color:#dcdfe4;background-color:#282c34><code class=language-rs data-lang=rs><span style=color:#c678dd>fn </span><span style=color:#61afef>range</span><span>(</span><span style=color:#e06c75>offsets</span><span>: </span><span style=color:#c678dd>&</span><span>[</span><span style=color:#c678dd>u32</span><span>], </span><span style=color:#e06c75>idx</span><span>: </span><span style=color:#c678dd>u16</span><span>) -> std::ops::Range<</span><span style=color:#c678dd>usize</span><span>> {
</span><span>  </span><span style=color:#c678dd>let</span><span> idx </span><span style=color:#c678dd>=</span><span> idx </span><span style=color:#c678dd>as usize</span><span>;
</span><span>  </span><span style=color:#c678dd>let</span><span> l </span><span style=color:#c678dd>=</span><span> offsets[idx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>0</span><span>] </span><span style=color:#c678dd>as usize</span><span>;
</span><span>  </span><span style=color:#c678dd>let</span><span> r </span><span style=color:#c678dd>=</span><span> offsets[idx </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>1</span><span>] </span><span style=color:#c678dd>as usize</span><span>;
</span><span>  l</span><span style=color:#c678dd>..</span><span>r
</span><span>}
</span><span>
</span><span style=color:#c678dd>struct </span><span>Decoder {
</span><span>  </span><span style=color:#e06c75>buffer</span><span>: Box<[</span><span style=color:#c678dd>u8</span><span>]>,
</span><span>  </span><span style=color:#e06c75>offsets</span><span>: Box<[</span><span style=color:#c678dd>u32</span><span>]>,
</span><span>}
</span><span>
</span><span style=color:#c678dd>impl </span><span>Decoder {
</span><span>  </span><span style=color:#c678dd>fn </span><span style=color:#61afef>new</span><span>(</span><span style=color:#e06c75>pairs</span><span>: </span><span style=color:#c678dd>&</span><span>[(</span><span style=color:#e06c75>u16</span><span>, </span><span style=color:#e06c75>u16</span><span>)]) -> </span><span style=color:#c678dd>Self </span><span>{
</span><span>    </span><span style=color:#c678dd>let mut</span><span> buffer: Vec<</span><span style=color:#c678dd>u8</span><span>> </span><span style=color:#c678dd>= </span><span>(</span><span style=color:#e5c07b>0</span><span style=color:#c678dd>..=</span><span style=color:#e5c07b>255</span><span>).</span><span style=color:#61afef>collect</span><span>();
</span><span>    </span><span style=color:#c678dd>let mut</span><span> offsets: Vec<</span><span style=color:#c678dd>u32</span><span>> </span><span style=color:#c678dd>= </span><span>(</span><span style=color:#e5c07b>0</span><span style=color:#c678dd>..=</span><span style=color:#e5c07b>255</span><span>).</span><span style=color:#61afef>collect</span><span>();
</span><span>    offsets.</span><span style=color:#61afef>reserve</span><span>(pairs.</span><span style=color:#61afef>len</span><span>() </span><span style=color:#c678dd>+ </span><span style=color:#e5c07b>1</span><span>);
</span><span>
</span><span>    </span><span style=color:#c678dd>for &</span><span>(left, right) </span><span style=color:#c678dd>in</span><span> pairs {
</span><span>      offsets.</span><span style=color:#61afef>push</span><span>(buffer.</span><span style=color:#61afef>len</span><span>() </span><span style=color:#c678dd>as u32</span><span>);
</span><span>      buffer.</span><span style=color:#61afef>extend_from_within</span><span>(</span><span style=color:#61afef>range</span><span>(</span><span style=color:#c678dd>&</span><span>offsets, left));
</span><span>      buffer.</span><span style=color:#61afef>extend_from_within</span><span>(</span><span style=color:#61afef>range</span><span>(</span><span style=color:#c678dd>&</span><span>offsets, right));
</span><span>    }
</span><span>    offsets.</span><span style=color:#61afef>push</span><span>(buffer.</span><span style=color:#61afef>len</span><span>() </span><span style=color:#c678dd>as u32</span><span>);
</span><span>
</span><span>    </span><span style=color:#c678dd>Self </span><span>{ buffer: buffer.</span><span style=color:#61afef>into</span><span>(), offsets: offsets.</span><span style=color:#61afef>into</span><span>() }
</span><span>  }
</span><span>
</span><span>  </span><span style=color:#c678dd>fn </span><span style=color:#61afef>decode</span><span>(</span><span style=color:#c678dd>&</span><span style=color:#e06c75>self</span><span>, </span><span style=color:#e06c75>token</span><span>: </span><span style=color:#c678dd>u16</span><span>) -> </span><span style=color:#c678dd>&</span><span>[</span><span style=color:#c678dd>u8</span><span>] {
</span><span>    </span><span style=color:#c678dd>&</span><span style=color:#e06c75>self</span><span>.buffer[</span><span style=color:#61afef>range</span><span>(</span><span style=color:#c678dd>&</span><span style=color:#e06c75>self</span><span>.offsets, token)]
</span><span>  }
</span><span>}
</span></code></pre><p>I mean, just look at the layout of this thing. Like it’s begging to be serialized. Usually the decoder is an auxiliary data structure and is usually instantiated on-the-fly instead of being serialized. But you can easily add serialization if you want to: just directly copy the buffers instead of traversing an array of arrays and collecting elements. I’m still deciding between serializing the entire <code>Decoder</code> or only the <code>pairs</code> and reconstructing the <code>Decoder</code> on-the-fly.<p>This streamlined the memory usage by reducing allocation overhead of small decoded token strings. Each lookup entry is now a single <code>u32</code> which is 4 bytes, instead of a <code>Box<[u8]></code> which is a pointer and a <code>usize</code>, so 16 bytes on 64-bit architectures. Using a single buffer also helps with cache locality, so decoding speed is likely to be improved.<h1 id=conclusion>Conclusion</h1><p>So that’s it for the decoding process. I think that it fits really well with the criteria that we established earlier. Decoding is relatively simple, and even the naive implementation is optimal in terms of time complexity. But even then we can still optimize for the memory access pattern, data layout, and allocation overhead.<p>The data layout of the decoder is also what I used to represent the bigram model for text generation, which is then cleverly constructed in linear time using a modified version of counting sort. But that and the whole text generation thing won’t be covered in this series. Next I’ll start talking about training the byte pair encoder, starting with a naive <code>O(n^2)</code> algorithm.</article></main><footer class=footer__container><svg viewbox="0 352.5 960 188.5" class=footer__split preserveaspectratio=none version=1.1 xmlns=http://www.w3.org/2000/svg xmlns:xlink=http://www.w3.org/1999/xlink><path d="M0 441L22.8 424.5C45.7 408 91.3 375 137 360.8C182.7 346.7 228.3 351.3 274 369.3C319.7 387.3 365.3 418.7 411.2 414.2C457 409.7 503 369.3 548.8 368C594.7 366.7 640.3 404.3 686 421.7C731.7 439 777.3 436 823 429.8C868.7 423.7 914.3 414.3 937.2 409.7L960 405L960 541L937.2 541C914.3 541 868.7 541 823 541C777.3 541 731.7 541 686 541C640.3 541 594.7 541 548.8 541C503 541 457 541 411.2 541C365.3 541 319.7 541 274 541C228.3 541 182.7 541 137 541C91.3 541 45.7 541 22.8 541L0 541Z" fill=#f7f9fb /><path d="M0 477L22.8 467C45.7 457 91.3 437 137 423.8C182.7 410.7 228.3 404.3 274 401.7C319.7 399 365.3 400 411.2 413.5C457 427 503 453 548.8 464.2C594.7 475.3 640.3 471.7 686 456.5C731.7 441.3 777.3 414.7 823 404.5C868.7 394.3 914.3 400.7 937.2 403.8L960 407L960 541L937.2 541C914.3 541 868.7 541 823 541C777.3 541 731.7 541 686 541C640.3 541 594.7 541 548.8 541C503 541 457 541 411.2 541C365.3 541 319.7 541 274 541C228.3 541 182.7 541 137 541C91.3 541 45.7 541 22.8 541L0 541Z" fill=#eff3f7 /><path d="M0 459L22.8 456.2C45.7 453.3 91.3 447.7 137 456.7C182.7 465.7 228.3 489.3 274 494.8C319.7 500.3 365.3 487.7 411.2 476C457 464.3 503 453.7 548.8 451.7C594.7 449.7 640.3 456.3 686 462.5C731.7 468.7 777.3 474.3 823 473.2C868.7 472 914.3 464 937.2 460L960 456L960 541L937.2 541C914.3 541 868.7 541 823 541C777.3 541 731.7 541 686 541C640.3 541 594.7 541 548.8 541C503 541 457 541 411.2 541C365.3 541 319.7 541 274 541C228.3 541 182.7 541 137 541C91.3 541 45.7 541 22.8 541L0 541Z" fill=#e8eef4 stroke=#e8eef4 /></svg><div class=footer__bg><div class=footer>© 2024 Zap (Huy-Giap Bui). Content on this site is licensed under <a href=//creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0</a>.</div></div></footer></div>