<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="Bui Huy Giap's personal website and blog" name=description><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon><title>
      
        
          My personal strategies for arena allocation | Zap's website
        
      
    </title><link href=/styles/fonts-critical.css rel=stylesheet><link onload="this.onload=null;this.rel='stylesheet';document.body.classList.add('font-loaded')" as=style href=/styles/fonts.css rel=preload><noscript><link href=/styles/fonts.css rel=stylesheet></noscript><style>h1{font-size:1.682rem}h2{font-size:1.414rem}h3{font-size:1.189rem}h4{font-size:1rem}h5{font-size:.841rem}h6{font-size:.707rem}</style><link href=/styles/base.css rel=stylesheet><link href=/styles/nav.css rel=stylesheet><link href=/styles/footer.css rel=stylesheet><link href=/rss.xml rel=alternate title=RSS type=application/rss+xml><link href=/styles/page.css rel=stylesheet><body><div class=nav__bg><div class=nav__container><nav class=nav><h3 class=nav__title><a class=nav__title__link href=/>Zap</a></h3><ul class=nav__links><li><a class=nav__link href=/works>Works</a><li><a class=nav__link href=/blog>Blog</a><li><a class=nav__link href=/about>About</a></ul></nav></div></div><div class=container><main class=content><header class=post-header><h1 class="post-header__title title">My personal strategies for arena allocation</h1><div class=post-header__meta><div class=post-header__data><svg viewbox="0 0 448 512" height=1em xmlns=http://www.w3.org/2000/svg><path d="M128 0c17.7 0 32 14.3 32 32V64H288V32c0-17.7 14.3-32 32-32s32 14.3 32 32V64h48c26.5 0 48 21.5 48 48v48H0V112C0 85.5 21.5 64 48 64H96V32c0-17.7 14.3-32 32-32zM0 192H448V464c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V192zm64 80v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V272c0-8.8-7.2-16-16-16H80c-8.8 0-16 7.2-16 16zm128 0v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V272c0-8.8-7.2-16-16-16H208c-8.8 0-16 7.2-16 16zm144-16c-8.8 0-16 7.2-16 16v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V272c0-8.8-7.2-16-16-16H336zM64 400v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V400c0-8.8-7.2-16-16-16H80c-8.8 0-16 7.2-16 16zm144-16c-8.8 0-16 7.2-16 16v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V400c0-8.8-7.2-16-16-16H208zm112 16v32c0 8.8 7.2 16 16 16h32c8.8 0 16-7.2 16-16V400c0-8.8-7.2-16-16-16H336c-8.8 0-16 7.2-16 16z"/></svg> Thu, Jun 20 2024</div><div class=post-header__data><svg viewbox="0 0 512 512" height=1em xmlns=http://www.w3.org/2000/svg><path d="M256 0a256 256 0 1 1 0 512A256 256 0 1 1 256 0zM232 120V256c0 8 4 15.5 10.7 20l96 64c11 7.4 25.9 4.4 33.3-6.7s4.4-25.9-6.7-33.3L280 243.2V120c0-13.3-10.7-24-24-24s-24 10.7-24 24z"/></svg><abbr title="1869 words"> 8 minutes read </abbr></div></div></header><article class=content__body><p>A drawback that people often mention when programming in C is the lack of RAII. This is sometimes good as it causes me to avoid small, random allocations and incentivize grouping data into large, contiguous memory regions to make them faster and more manageable. However, small, random allocations are sometimes unavoidable, and there should still be a way to manage them. Instead of grouping data into arrays, we can instead group data by their <em>lifetime</em>, and this is where the “arena” allocator comes in.</p><span id=continue-reading></span><p>The arena allocator has many names, such as linear allocator and region-based allocator. It’s not a specific allocator (like jemalloc), but a method for allocating and managing memory. In my opinion, the management part is more important than the allocation part. Because it supposedly solves some shortcomings of C. The main idea is that instead of deallocating individual memory, the arena allocator frees everything at once, hence all items have the same lifetime.<h1 id=allocator-features>Allocator features</h1><p>Another benefit of arenas are its simplicity and speed, but I won’t capitalize too much on them. My implementation will be a bit heavier than the typical arena allocator, but it supports the following features:<ol><li>No maximum capacity, use as much memory as it needs<li><code>O(1)</code> Bulk reset (mark all allocated memory as free for future allocation)<li>Can allocate items of any size<li>Reallocation</ol><h2 id=dynamic-capacity>Dynamic capacity</h2><p>To support the first feature, the allocator needs a “back-end” allocator, instead of just using a static buffer to allocate memory from. It doesn’t have to be a general-purpose allocator; you can use a page-level allocator, such as <code>mmap</code>. We will call the memory allocated with the back-end allocator “chunks”. Because the allocator needs to free everything at once, it has to keep track of all chunks, so they are stored in a linked list. When there isn’t enough memory to fulfill an allocation request, a new chunk is created, appended to the end of the list, and used for subsequent allocations.<figure><img alt="allocation visualization" src=./small-allocation.svg><figcaption>Example of the allocator allocating memory</figcaption></figure><p>In the visualization above, the green space is the memory available for allocation. The red space is the previously allocated memory. And the white space is the unused (fragmented) memory. We ignored some details here, such as alignment, but the idea is still the same. To reduce the proportion of the fragmented region, we can make the chunk size a lot bigger than the average allocation size.<h2 id=reset-operation>Reset operation</h2><p>This operation is the main feature of the allocator. It’s like garbage collection, except it assumes that everything is no longer being used and collects everything. This assumption greatly simplifies the problem, but it requires a paradigm shift in memory management. The easiest way to implement bulk reset is to traverse the linked list and free all chunks. But as mentioned earlier, the operation needs to have constant time complexity. To achieve this and make allocating chunks more efficient, we keep track of a list of free chunks. We can use the linked list structure of the chunk for the free list itself!<figure><img alt="free list visualization" src=free-list.svg><figcaption>Example of getting a chunk from the free list</figcaption></figure><p>With this new “chunk allocator”, the back-end allocator is only called when the free list is empty, so allocation speed is further improved. Now, to mark all allocated memory as freed, all we have to do is concatenate the in-use list with the free list and set it as the new free list. The in-use list then becomes empty. To concatenate two lists in <code>O(1)</code>, singly linked list is not enough, so I added a tail pointer to the in-use list. Concatenation is now as simple as linking the tail of the in-use list with the head of the free list.<h2 id=arbitrary-sized-allocation>Arbitrary sized allocation</h2><p>With this architecture, you might be wondering: What if the allocation request is bigger than the chunk size? The solution is very simple: we allocate an entire chunk to fulfill the request. To reduce fragmentation, the new chunk is stored <em>behind</em> the current chunk, so the current chunk and its available memory can still be used.<figure><img alt="big allocation visualization" src=big-allocation.svg><figcaption>Example of allocation a region larger than the chunk size</figcaption></figure><p>This means that the chunks are now variable-sized. So I had to also store the capacity of each chunk so that a big chunk can be reused for multiple small allocations. But how do you allocate a chunk big enough for the request? Here are some ways of solving this:<p><strong>Option 1:</strong> Store the static-sized chunks and the variable-sized chunks in two different free lists with different data structures. The static-sized chunks can still use a linked list, but the variable-sized chunks can use either a <a href=//en.wikipedia.org/wiki/Mergeable_heap>meldable heap</a> or a <a href=//en.wikipedia.org/wiki/Search_tree>search tree</a>. These data structures try to avoid invoking the back-end allocator. The heap does this by always getting the biggest region, while the search tree can get the smallest region that fits the request. Obviously, the search tree is better for reducing fragmentation than the heap, but remember that when we reset the arena, we need to combine the in-use list and the free list, which the meldable heap can perform more efficiently (hence the name).<p><strong>Option 2:</strong> Store the static-sized chunks and the variable-sized chunks in the same linked list. This method has the benefit of being simple and still supports <code>O(1)</code> bulk reset, but it will invoke the back-end allocator more often. To allocate a big chunk with this method, when the free list is not empty, detach the first chunk from the free list. If it doesn’t fit the request, deallocate it and use a chunk allocated with the back-end allocator; otherwise, use it. This means that every time the back-end allocates a chunk, a chunk in the free list is deallocated.<p>Because I prioritized <code>O(1)</code> bulk reset, I chose to implement the second option. Yes, the <a href=//en.wikipedia.org/wiki/Fibonacci_heap>Fibonacci heap</a> can merge in <code>O(1)</code> amortized time complexity, but I don’t think the complexity and overhead of the Fibonacci heap are worth it. Another interesting data structure is the splay tree, which can adapt to the allocation pattern, but merging two splay trees is an <code>O(n log n)</code> operation, which is not ideal. For some other usage, these data structures may be better, but for now, I’m using the easy method.<h2 id=resize-allocated-memory>Resize allocated memory</h2><p>For me, this operation is very important, as I frequently use dynamic array-based data structures. It might be better to let dynamic arrays have their own lifetimes, but recently I encountered a situation that requires me to group the lifetimes of multiple dynamic arrays and other objects. It’s an operation vital enough for me to drastically change the structure of the allocator to accommodate: I have to switch to a doubly linked list. There are <strong>3 cases</strong> for resizing:<h3 id=case-1>Case 1</h3><p>In this case, the memory region is bigger than the fixed chunk size. From the allocation strategy above, the region has an entire chunk for itself. So to resize, we can just resize the chunk with the back-end allocator. Then we have to update the pointers correctly, which is why we need a doubly linked list.<figure><img alt="resize case 1 visualization" src=resize-case1.svg><figcaption>Example of resizing a region larger than the chunk size</figcaption></figure><p>As shown in the figure above, the resized chunk may be stored somewhere else in memory, so the next and previous nodes in the list are invalidated. So we have to follow the pointers to get to them and link them back. This operation can work without, but benefits from, a back-end allocator with an efficient resize operation, such as <code>realloc</code>.<h3 id=case-2>Case 2</h3><p>In this case, the memory region that you want to resize is the previously allocated region, and there is enough space in the current chunk to expand it. Resizing now is just shifting the border between the allocated region and the available region. The only modification required for this is that we now have to store the pointer to the previous allocation.<figure><img alt="resize case 2 visualization" src=resize-case2.svg><figcaption>Example of resizing the last allocation</figcaption></figure><p>This may sound specific, but resizing the last allocation is more frequently used than you think. It’s common to initialize an array by pushing a variable amount of items into it, then never change the size afterward. If the array also happens to be the first region in the chunk, when the size exceeds the fixed chunk size, it automatically switches to case 1 resizing without any memory fragmentation.<h3 id=case-3>Case 3</h3><p>When the conditions for the previous cases are not met, we have to fall back to a simple resizing strategy: Allocate a new region of memory and copy the data from the previous memory to the new one. This is an expensive operation; the old chunk can’t be reused (for future allocation), so it should be avoided. But in some cases (like for dynamic arrays), it is tolerable to do this.<h1 id=programming-interface>Programming Interface</h1><p>So those are the characteristics of the allocator. It’s quite big and complicated, so I won’t show the full code of it. But I’ll show what using the allocator feels like. This is the entire interface:<pre class=language-c data-lang=c style=color:#dcdfe4;background-color:#282c34><code class=language-c data-lang=c><span>Arena </span><span style=color:#61afef>Arena_create</span><span>(</span><span style=color:#c678dd>void</span><span>);
</span><span style=color:#c678dd>void *</span><span style=color:#61afef>Arena_alloc</span><span>(Arena </span><span style=color:#c678dd>*</span><span style=color:#e06c75>arena</span><span>, size_t </span><span style=color:#e06c75>size</span><span>);
</span><span style=color:#c678dd>void *</span><span style=color:#61afef>Arena_resize</span><span>(Arena </span><span style=color:#c678dd>*</span><span style=color:#e06c75>arena</span><span>, </span><span style=color:#c678dd>void *</span><span style=color:#e06c75>ptr</span><span>, size_t </span><span style=color:#e06c75>old_size</span><span>, size_t </span><span style=color:#e06c75>new_size</span><span>);
</span><span style=color:#c678dd>void </span><span style=color:#61afef>Arena_reset</span><span>(Arena </span><span style=color:#c678dd>*</span><span style=color:#e06c75>arena</span><span>);
</span><span style=color:#c678dd>void </span><span style=color:#61afef>Arena_destroy</span><span>(Arena </span><span style=color:#e06c75>arena</span><span>);
</span></code></pre><p>If you’re used to the general-purpose, built-in libc allocator, you might find it weird that we have to create an arena object and pass it around to allocate memory. Why not just have a global allocator and expose it with some functions? Personally, I prefer passing things with internal states around, but even without that, it is required so that multiple arenas with different lifetimes can be created and used independently. An example of a function that allocates memory now looks like this:<pre class=language-c data-lang=c style=color:#dcdfe4;background-color:#282c34><code class=language-c data-lang=c><span>Data </span><span style=color:#c678dd>*</span><span style=color:#61afef>create_data</span><span>(</span><span style=color:#c678dd>...</span><span>, Arena </span><span style=color:#c678dd>*</span><span style=color:#e06c75>arena</span><span>);
</span></code></pre><p>I think it’s cool that now the function signature not only tells us that it allocates memory, but also tell us what the lifetime of the allocated memory is. Passing an arena into a function is like telling it to move data from its lifetime—which ends when the function returns—to the lifetime of the arena, using pointers that point to memory allocated by it.<h1 id=closing>Closing</h1><p>But why do we need this lifetime stuff? Well, I’m not the person to tell you this. This article just describes my approach to arena allocation and does not propose it. I think that you should decide whether you need to use arena allocators, and if you do, here is one way of doing it.<p>Personally, I don’t like this allocator too much. It’s complex and depends on another allocator. But it’s extremely generic and has many features that I find useful. I’m still experimenting with this method to see if it’s stable and actually reduces the complexity of memory management.<p><strong>Edit:</strong> The allocator’s implementation is available <a href=//github.com/ziap/arena>here</a>.</article></main><footer class=footer__container><svg viewbox="0 352.5 960 188.5" class=footer__split preserveaspectratio=none version=1.1 xmlns=http://www.w3.org/2000/svg xmlns:xlink=http://www.w3.org/1999/xlink><path d="M0 441L22.8 424.5C45.7 408 91.3 375 137 360.8C182.7 346.7 228.3 351.3 274 369.3C319.7 387.3 365.3 418.7 411.2 414.2C457 409.7 503 369.3 548.8 368C594.7 366.7 640.3 404.3 686 421.7C731.7 439 777.3 436 823 429.8C868.7 423.7 914.3 414.3 937.2 409.7L960 405L960 541L937.2 541C914.3 541 868.7 541 823 541C777.3 541 731.7 541 686 541C640.3 541 594.7 541 548.8 541C503 541 457 541 411.2 541C365.3 541 319.7 541 274 541C228.3 541 182.7 541 137 541C91.3 541 45.7 541 22.8 541L0 541Z" fill=#f7f9fb /><path d="M0 477L22.8 467C45.7 457 91.3 437 137 423.8C182.7 410.7 228.3 404.3 274 401.7C319.7 399 365.3 400 411.2 413.5C457 427 503 453 548.8 464.2C594.7 475.3 640.3 471.7 686 456.5C731.7 441.3 777.3 414.7 823 404.5C868.7 394.3 914.3 400.7 937.2 403.8L960 407L960 541L937.2 541C914.3 541 868.7 541 823 541C777.3 541 731.7 541 686 541C640.3 541 594.7 541 548.8 541C503 541 457 541 411.2 541C365.3 541 319.7 541 274 541C228.3 541 182.7 541 137 541C91.3 541 45.7 541 22.8 541L0 541Z" fill=#eff3f7 /><path d="M0 459L22.8 456.2C45.7 453.3 91.3 447.7 137 456.7C182.7 465.7 228.3 489.3 274 494.8C319.7 500.3 365.3 487.7 411.2 476C457 464.3 503 453.7 548.8 451.7C594.7 449.7 640.3 456.3 686 462.5C731.7 468.7 777.3 474.3 823 473.2C868.7 472 914.3 464 937.2 460L960 456L960 541L937.2 541C914.3 541 868.7 541 823 541C777.3 541 731.7 541 686 541C640.3 541 594.7 541 548.8 541C503 541 457 541 411.2 541C365.3 541 319.7 541 274 541C228.3 541 182.7 541 137 541C91.3 541 45.7 541 22.8 541L0 541Z" fill=#e8eef4 stroke=#e8eef4 /></svg><div class=footer__bg><div class=footer>© 2024 Zap (Huy-Giap Bui). Content on this site is licensed under <a href=//creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0</a>.</div></div></footer></div>